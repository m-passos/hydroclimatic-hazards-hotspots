{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74713058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolation and calculation of Drought Indices using precipitation data, hotspots and trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a09e362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel, ExpSineSquared, ConstantKernel, RBF, Matern, RationalQuadratic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7895a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time as time2\n",
    "import os\n",
    "# Interpolate spatially using GaussianProcessRegressor (modified Kriging)\n",
    "# create a grid of latitude and longitude coordinates\n",
    "lat_grid = np.linspace(55.3376, 69.1763, 100) \n",
    "lon_grid = np.linspace(11, 24.113, 100)\n",
    "nx = len(lon_grid)\n",
    "ny = len(lat_grid)\n",
    "times=df_all_mon.ref.unique()\n",
    "#datesout=pd.to_datetime(times)\n",
    "dataout=np.zeros((len(times),ny,nx))\n",
    "#dataout=np.load('dataout_16000.npy')\n",
    "out_rsquared=[]\n",
    "in_rsquared=[]\n",
    "\n",
    "t=0\n",
    "\n",
    "merged_df=pd.merge(df_stations[['id', 'latitude', 'longitude']], df_all_mon, left_on='id', right_on='station_id')\n",
    "merged_df=merged_df.sort_values('ref', ascending=True)\n",
    "start = time2.time()\n",
    "for n in range(len(times)):\n",
    "    time=times[n]\n",
    "    temp_subset = merged_df[merged_df.ref==time]\n",
    "    # Split data into testing and training sets\n",
    "    merged_train = temp_subset[['latitude','longitude']]\n",
    "    coords_train, coords_test, value_train, value_test = train_test_split(merged_train, temp_subset['value'], test_size = 0.20, random_state = 42)\n",
    "    coords_train_wgs = [np.array(xy) for xy in zip(coords_train.longitude.values, coords_train.latitude.values)]\n",
    "    coords_test_wgs = [np.array(xy) for xy in zip(coords_test.longitude.values, coords_test.latitude.values)]\n",
    "    XX_sk_krig, YY_sk_krig = np.mgrid[lon_grid.min():lon_grid.max():100j, lat_grid.min():lat_grid.max():100j]\n",
    "    # Create 2-D array of the coordinates (paired) of each cell in the mesh grid\n",
    "    positions_sk_krig = np.vstack([XX_sk_krig.ravel(), YY_sk_krig.ravel()]).T\n",
    "    kernel2= RationalQuadratic()\n",
    "    kernel= RBF()\n",
    "\n",
    "    # Define the hyperparameters\n",
    "    length_scale = 0.1\n",
    "    alpha = 0.1\n",
    "\n",
    "    try:\n",
    "        gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=3)\n",
    "        # Fit kernel density estimator to coordinates and values\n",
    "        gp.fit(coords_train_wgs, value_train)\n",
    "    except:\n",
    "        gp = GaussianProcessRegressor(kernel=kernel2, n_restarts_optimizer=3)\n",
    "        # Fit kernel density estimator to coordinates and values\n",
    "        gp.fit(coords_train_wgs, value_train)\n",
    "        print('error')\n",
    "    # Evaluate the model on coordinate pairs\n",
    "    Z_sk_krig, std_prediction = gp.predict(positions_sk_krig, return_std=True)\n",
    "    # Reshape the data to fit mesh grid\n",
    "    interp_temps = Z_sk_krig.reshape(XX_sk_krig.shape)\n",
    "    # Generate in-sample R^2\n",
    "    in_r_squared_sk_krig = gp.score(coords_train_wgs, value_train)\n",
    "    print(\"Scikit-Learn Kriging in-sample r-squared: {}\".format(round(in_r_squared_sk_krig, 2)))\n",
    "    # Generate out-of-sample R^2\n",
    "    out_r_squared_sk_krig = gp.score(coords_test_wgs, value_test)\n",
    "    print(\"Scikit-Learn Kriging out-of-sample r-squared: {}\".format(round(out_r_squared_sk_krig, 2)))\n",
    "    Z_sk_krig[Z_sk_krig<0]=0\n",
    "    Z_sk_krig[Z_sk_krig>np.nanmax(value_train)]=np.nanmax(value_train)\n",
    "    # Check for stable results\n",
    "    #if np.nanmax(interp_temps)<=upper_limit and np.nanmin(interp_temps)>=lower_limit:\n",
    "    if out_r_squared_sk_krig>-1:\n",
    "        in_rsquared.append(in_r_squared_sk_krig)\n",
    "        out_rsquared.append(out_r_squared_sk_krig)\n",
    "        dataout[t,:]=interp_temps\n",
    "        end = time2.time()\n",
    "        print('Time elapsed (s): '+ str(end - start))\n",
    "        #break\n",
    "    else:\n",
    "        #dataout[t,:]=np.nan\n",
    "        print('Low correlation, trying again...')\n",
    "        kernel= RationalQuadratic()\n",
    "        gp = GaussianProcessRegressor(kernel=kernel, alpha=0.1, n_restarts_optimizer=5)\n",
    "        gp.fit(coords_train_wgs, value_train)\n",
    "        Z_sk_krig, std_prediction = gp.predict(positions_sk_krig, return_std=True)\n",
    "        Z_sk_krig[Z_sk_krig<0]=0\n",
    "        Z_sk_krig[Z_sk_krig>np.nanmax(value_train)]=np.nanmax(value_train)\n",
    "        interp_temps = Z_sk_krig.reshape(XX_sk_krig.shape)\n",
    "        in_r_squared_sk_krig = gp.score(coords_train_wgs, value_train)\n",
    "        print(\"Scikit-Learn Kriging in-sample r-squared: {}\".format(round(in_r_squared_sk_krig, 2)))\n",
    "        out_r_squared_sk_krig = gp.score(coords_test_wgs, value_test)\n",
    "        print(\"Scikit-Learn Kriging out-of-sample r-squared: {}\".format(round(out_r_squared_sk_krig, 2)))\n",
    "        #kernel= RationalQuadratic(0.1) * (DotProduct(0.1))**2\n",
    "        if np.nanmax(interp_temps)<=upper_limit and np.nanmin(interp_temps)>=lower_limit:\n",
    "            dataout[t,:]=interp_temps \n",
    "            in_rsquared.append(in_r_squared_sk_krig)\n",
    "            out_rsquared.append(out_r_squared_sk_krig)\n",
    "        else:\n",
    "            #dataout[t,:]=np.nan\n",
    "            print('error')\n",
    "            break\n",
    "    print(time)\n",
    "    print(t)\n",
    "    print('Mean interpolated values: '+str(np.mean(dataout[t,:])))\n",
    "    t=t+1\n",
    "print(np.mean(in_rsquared))\n",
    "print(np.mean(out_rsquared))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cd9a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask out values outside country boundaries\n",
    "from shapely.geometry import Polygon, Point\n",
    "from cartopy.io import shapereader\n",
    "import geopandas\n",
    "def rect_from_bound(xmin, xmax, ymin, ymax):\n",
    "    \"\"\"Returns list of (x,y)'s for a rectangle\"\"\"\n",
    "    xs = [xmax, xmin, xmin, xmax, xmax]\n",
    "    ys = [ymax, ymax, ymin, ymin, ymax]\n",
    "    return [(x, y) for x, y in zip(xs, ys)]\n",
    "# Mask from natural earth (more below)\n",
    "# request data for use by geopandas\n",
    "resolution = '50m'\n",
    "category = 'cultural'\n",
    "name = 'admin_0_countries'\n",
    "\n",
    "shpfilename = shapereader.natural_earth(resolution, category, name)\n",
    "df = geopandas.read_file(shpfilename)\n",
    "\n",
    "# get geometry and polygon of a country\n",
    "poly = [df.loc[df['ADMIN'] == 'Sweden']['geometry'].values[0]]\n",
    "exts = [min(lon_grid)-1, max(lon_grid)+1, min(lat_grid)-1, max(lat_grid)+1]\n",
    "msk = Polygon(rect_from_bound(*exts)).difference( poly[0].simplify(0.01) )\n",
    "# create boolean mask where True=inside polygon\n",
    "mask=np.zeros((len(lat_grid),len(lon_grid)),dtype=bool)\n",
    "for i, y in enumerate(lat_grid):\n",
    "    for j, x in enumerate(lon_grid):\n",
    "        mask[i,j]=Point(x,y).within(msk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fceb6670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean interpolated data to remove values out of Sweden\n",
    "times=df_all_mon.ref.unique()\n",
    "for t in range(len(times)):\n",
    "    dataout[t,:]=np.ma.array(dataout[t,:].T, mask = mask).filled(np.nan) # Transpose to correct plot for grid\n",
    "    #t=t+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8702402",
   "metadata": {},
   "outputs": [],
   "source": [
    "precip_mon.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502605f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b02e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save netcdf data (xarray)\n",
    "import xarray as xr\n",
    "\n",
    "# Create a 1d array for the time axis\n",
    "times=df_all_mon.ref.unique()\n",
    "# Create a DataArray from the dataout numpy array, with the time axis as the first dimension\n",
    "da = xr.DataArray(dataout, dims=('time', 'lat', 'lon'), coords={'time': times})\n",
    "da['lat']=lat_grid\n",
    "da['lon']=lon_grid\n",
    "# Convert the DataArray to a Dataset\n",
    "ds = da.to_dataset(name='precipitation')\n",
    "\n",
    "# Save the Dataset to a netCDF file\n",
    "ds.to_netcdf('precip-12mon.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce9ba64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#precip_mon = xr.open_dataset('precip-1mon.nc')\n",
    "precip_yr = xr.open_dataset('precip-12mon.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06693e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmax(precip_mon['precipitation'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0805c4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a113a820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When did the maximum rainfall occured?\n",
    "np.where(precip_mon['precipitation'].values==np.nanmax(precip_mon['precipitation'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20d5666",
   "metadata": {},
   "outputs": [],
   "source": [
    "precip['time'].values[13791]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1107c189",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_grid[93], lon_grid[55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741194f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot average precipitation rates\n",
    "precip_values = precip_mon['precipitation'].values\n",
    "# Mask the zero values in the array using numpy.where()\n",
    "masked_values = np.where(precip_values != 0, precip_values, np.nan)\n",
    "# Calculate the mean of the non-zero values using numpy.nanmean()\n",
    "precip_mean = np.nanmean(masked_values, axis=0)\n",
    "plot_map(precip_mean, 'Average Monthly Precipitation (mm)', 0, 5, 'imshow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5d91e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "precip_mon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c78080",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(precip.time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cb2af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=365 # conversion for 12-monthly precipitation\n",
    "precip = xr.open_dataset('precipitation.nc')#.round()\n",
    "precip_rolling= precip.rolling(time=f, min_periods=f).sum()\n",
    "# calculate daily rolling sums with a window size of f days\n",
    "precip_rolling = precip.rolling(time=f, min_periods=1).sum()\n",
    "precip_yr = precip_rolling\n",
    "#precip_yr = xr.open_dataset('precip-12mon.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a8e1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "lat_grid = np.linspace(55.3376, 69.1763, 100) \n",
    "lon_grid = np.linspace(11, 24.113, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42d5dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import kstest, gamma\n",
    "def spi_transform(H):\n",
    "    #print(H)\n",
    "    norm=[]\n",
    "    c0= 2.515517\n",
    "    c1= 0.802853\n",
    "    c2= 0.010328\n",
    "    d1= 1.432788\n",
    "    d2= 0.189269\n",
    "    d3= 0.001308\n",
    "    for h in H:\n",
    "        #print(h)\n",
    "        if h<=0.5:\n",
    "            t=np.sqrt(np.log(1/h**2))\n",
    "            spi_h=-(t-(c0+c1*t+c2*t**2)/(1+d1*t+d2*t**2+d3*t**3))\n",
    "        elif h>0.5: #and h<1:\n",
    "            t=np.sqrt(np.log(1/((1-h)**2)))\n",
    "            spi_h=+(t-(c0+c1*t+c2*t**2)/(1+d1*t+d2*t**2+d3*t**3))\n",
    "        else:\n",
    "            spi_h=np.nan\n",
    "        norm.append(spi_h)\n",
    "    return norm\n",
    "#from standard_precip.spi import SPI\n",
    "#from standard_precip.utils import plot_index\n",
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return array[idx]\n",
    "# Test calculating SPI at one point\n",
    "def SPI_point(lat, lon, precip_mon):\n",
    "    #dist=gamma\n",
    "    cutoff=0\n",
    "    lat_near= find_nearest(precip_mon.lat.values, lat)\n",
    "    lon_near= find_nearest(precip_mon.lon.values, lon)\n",
    "    rf_data_point = pd.DataFrame(columns=['time', 'precipitation'])\n",
    "    rf_data_point['precipitation'] = precip_mon.sel(lat=lat_near, lon=lon_near)['precipitation'].values\n",
    "    rf_data_point['time'] = precip_mon.sel(lat=lat_near, lon=lon_near)['time'].values\n",
    "    #print(rf_data_point.max())\n",
    "    if rf_data_point['precipitation'].mean()>0: # check if grid point is inside Sweden\n",
    "        #rf_data_point= rf_data_point[rf_data_point['daily precipitation']>=cutoff]\n",
    "        df_spi_point_sort= rf_data_point.copy()\n",
    "        df_spi_point_sort= df_spi_point_sort.sort_values(by='precipitation', ascending=True)\n",
    "        #df_spi_point_sort= df_spi_point_sort[df_spi_point_sort['daily precipitation']>cutoff]\n",
    "        # Plot histogram of monthly precipitation\n",
    "        pre_dist= rf_data_point['precipitation'].values\n",
    "        gamma_fit, cdf_dist, norm_ppf = fit_gamma(pre_dist, 0) \n",
    "        df_spi_point_sort['SPI']=norm_ppf\n",
    "        df_spi_point_sort=df_spi_point_sort.sort_values(by='time', ascending=True)\n",
    "    else:\n",
    "        df_spi_point_sort = pd.DataFrame(columns=['SPI'])\n",
    "        df_spi_point_sort['SPI'] = [np.nan for x in precip_mon.time.values]\n",
    "    return df_spi_point_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f5e7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=365 # SPEI12\n",
    "tmax_mon= tmax_ds.rolling(time=f, min_periods=f).sum().resample(time='1M').first()\n",
    "tmin_mon= tmin_ds.rolling(time=f, min_periods=f).sum().resample(time='1M').first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e08010",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hargreaves (1982)\n",
    "def hargreaves(Tmin, Tmax, date, latitude):\n",
    "    # Computation of extra-terrestrial solar radiation\n",
    "    doy = (date - np.datetime64(date.astype('datetime64[Y]'))) / np.timedelta64(1, 'D') + 1\n",
    "    #doy = date.dayofyear\n",
    "    dr = 1 + 0.033 * np.cos(2 * np.pi * doy/365) # Inverse relative distance Earth-Sun\n",
    "    phi = np.pi / 180 *  latitude # Latitude in radians\n",
    "    d = 0.409 * np.sin((2 * np.pi * doy/365) - 1.39) # Solar declination\n",
    "    omega = np.arccos(-np.tan(phi) * np.tan(d)) # Sunset hour angle\n",
    "    Gsc = 0.0820 # Solar constant\n",
    "    Ra = 24 * 60 / np.pi * Gsc * dr * (omega * np.sin(phi) * np.sin(d) + np.cos(phi) * np.cos(d) * np.sin(omega))\n",
    "    Tm = (Tmin + Tmax)/2 # temperature in celsius degrees\n",
    "    PET = 0.0023 * Ra * (Tm + 17.8) * (Tmax - Tmin)**0.5\n",
    "    Evap_m = PET / (2.45 * 1000) # output in m/d\n",
    "    Evap_mm = Evap_m*1000 # convert to mm/d\n",
    "    return Evap_mm \n",
    "def SPEI_point(lat, lon, precip_mon, tmax_data, tmin_data, time_daily, f, i, j):\n",
    "    #dist=gamma\n",
    "    cutoff=0\n",
    "    lat_near= find_nearest(precip_mon.lat.values, lat)\n",
    "    lon_near= find_nearest(precip_mon.lon.values, lon)\n",
    "    rf_data_point = pd.DataFrame(columns=['time', 'precipitation'])\n",
    "    rf_data_point['precipitation'] = precip_mon.sel(lat=lat_near, lon=lon_near)['precipitation'].values\n",
    "    rf_data_point['time'] = precip_mon.sel(lat=lat_near, lon=lon_near)['time'].values\n",
    "    #print(rf_data_point.max())\n",
    "    if rf_data_point['precipitation'].mean()>0: # check if grid point is inside Sweden\n",
    "        pet_daily=[]\n",
    "        # Extract temperature and time data for grid point\n",
    "        tmax_daily = tmax_data[:,i,j]\n",
    "        tmin_daily = tmin_data[:,i,j]\n",
    "        for i in range(len(time_daily)):\n",
    "            Tmax=tmax_daily[i]\n",
    "            Tmin=tmin_daily[i]\n",
    "            date=time_daily[i]\n",
    "            pet=hargreaves(Tmin, Tmax, date, lat_near)\n",
    "            if np.isnan(pet):\n",
    "                pet_daily.append(0)\n",
    "            else:\n",
    "                pet_daily.append(pet)\n",
    "        pet_df=pd.DataFrame(columns=('time','pet'))\n",
    "        pet_df['time']=pd.to_datetime(time_daily)\n",
    "        pet_df['pet']=pet_daily\n",
    "        pet_mon=pet_df.set_index('time').rolling(f, min_periods=f).sum().resample('1M').first()\n",
    "        #pet_mon=pet_mon.fillna(0)\n",
    "        rf_data_point['pre_excess']= rf_data_point['precipitation'].values-pet_mon['pet'].values\n",
    "        df_spi_point_sort= rf_data_point.copy()\n",
    "        df_spi_point_sort= df_spi_point_sort.sort_values(by='pre_excess', ascending=True)\n",
    "        # Plot histogram of monthly precipitation excess\n",
    "        pre_dist= rf_data_point['pre_excess'].values\n",
    "        #print(pet_mon['pet'])\n",
    "        pre_dist = pre_dist[~np.isnan(pre_dist)]\n",
    "        pdf_dist, cdf_dist, norm_ppf = fit_pearson3(pre_dist) \n",
    "        nan_array = np.empty(len(df_spi_point_sort.index) - len(norm_ppf))\n",
    "        nan_array[:] = np.nan\n",
    "        norm_ppf = np.concatenate((nan_array, norm_ppf))\n",
    "        df_spi_point_sort['SPEI']=norm_ppf\n",
    "        df_spi_point_sort=df_spi_point_sort.sort_values(by='time', ascending=True)\n",
    "    else:\n",
    "        df_spi_point_sort = pd.DataFrame(columns=['SPEI'])\n",
    "        df_spi_point_sort['SPEI'] = [np.nan for x in precip_mon.time.values]\n",
    "    return df_spi_point_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f278ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_near= lat_grid[j]\n",
    "lon_near= lon_grid[i]\n",
    "rf_data_point = pd.DataFrame(columns=['time', 'precipitation'])\n",
    "rf_data_point['precipitation'] = precip_mon.sel(lat=lat_near, lon=lon_near)['precipitation'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516f429a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Potential Evapotranspiration at all points (not used)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Import data\n",
    "tmax_data = tmax_ds['maximum daily air temperature'].values\n",
    "tmin_data = tmin_ds['maximum daily air temperature'].values\n",
    "time_data = tmax_ds['time'].values\n",
    "\n",
    "# Define grid\n",
    "lat_grid = np.linspace(55.3376, 69.1763, 100)\n",
    "lon_grid = np.linspace(11, 24.113, 100)\n",
    "\n",
    "# Initialize PET array\n",
    "pet_mon = np.zeros((len(tmax_ds.time.values), 100, 100))\n",
    "\n",
    "# Loop over grid points\n",
    "for i in range(len(lat_grid)):\n",
    "    for j in range(len(lon_grid)):\n",
    "        print(i,j)\n",
    "        # Extract temperature and time data for grid point\n",
    "        tmax_daily = tmax_data[:,i,j]\n",
    "        tmin_daily = tmin_data[:,i,j]\n",
    "        time_daily = time_data[:,i,j]\n",
    "        \n",
    "        # Check if grid point is inside Sweden\n",
    "        if np.mean(tmax_daily) > 0:\n",
    "            # Compute daily PET values\n",
    "            pet_daily = np.array([hargreaves(Tmin, Tmax, date, lat_near) for Tmin, Tmax, date in zip(tmin_daily, tmax_daily, time_daily)])\n",
    "            \n",
    "            # Convert to DataFrame and resample to monthly values\n",
    "            pet_df = pd.DataFrame({'time': time_daily, 'pet': pet_daily})\n",
    "            pet_df['time'] = pd.to_datetime(pet_df['time'])\n",
    "            pet_mon[:, i, j] = pet_df.set_index('time').rolling(f, min_periods=f).sum().resample('1M').first()['pet'].values\n",
    "        else:\n",
    "            pet_mon[:, i, j] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c65aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute X-month SPEI at all points \n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "precip_per=precip_yr\n",
    "#tmax_per=tmax_mon\n",
    "#tmin_per=tmin_mon\n",
    "# Import data\n",
    "tmax_per = tmax_ds['maximum daily air temperature'].values\n",
    "tmin_per = tmin_ds['maximum daily air temperature'].values\n",
    "time_per = tmax_ds['time'].values\n",
    "f=365\n",
    "# Calculate SPEI for all points\n",
    "#SPEI_data = np.zeros((len(precip_per.time.values), 100, 100))\n",
    "lat_grid = np.linspace(55.3376, 69.1763, 100) \n",
    "lon_grid = np.linspace(11, 24.113, 100)\n",
    "for i in range(len(lat_grid)):\n",
    "    for j in range(len(lon_grid)):\n",
    "        print(i,j)\n",
    "        df_spi_point = SPEI_point(lat_grid[i], lon_grid[j], precip_per, tmax_per, tmin_per, time_per, f, i, j) \n",
    "        SPEI_data[:,i,j] = df_spi_point['SPEI'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd83e5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute X-month SPI at all points \n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "precip_per=precip_yr\n",
    "# Calculate SPI for all points\n",
    "SPI_data = np.zeros((len(precip_per.time.values), 100, 100))\n",
    "lat_grid = np.linspace(55.3376, 69.1763, 100) \n",
    "lon_grid = np.linspace(11, 24.113, 100)\n",
    "for i in range(len(lat_grid)):\n",
    "    for j in range(len(lon_grid)):\n",
    "        print(i,j)\n",
    "        df_spi_point = SPI_point(lat_grid[i], lon_grid[j], precip_per) \n",
    "        SPI_data[:,i,j] = df_spi_point['SPI'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea37cd05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bbd202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save SPI/SPEI data to file\n",
    "import xarray as xr\n",
    "\n",
    "# Create a 1d array for the time axis\n",
    "times = precip_per.time.values\n",
    "\n",
    "# Create a DataArray from the dataout numpy array, with the time axis as the first dimension\n",
    "da = xr.DataArray(SPEI_data, dims=('time', 'lat', 'lon'), coords={'time': times})\n",
    "da['lat']=lat_grid\n",
    "da['lon']=lon_grid\n",
    "# Convert the DataArray to a Dataset\n",
    "ds = da.to_dataset(name='SPEI')\n",
    "\n",
    "# Save the Dataset to a netCDF file\n",
    "#ds.to_netcdf('SPEI112.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7153134",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562d6ae3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fe54d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat= 59.3293\n",
    "lon= 18.0686\n",
    "lat_near= find_nearest(precip_mon.lat.values, lat)\n",
    "lon_near= find_nearest(precip_mon.lon.values, lon)\n",
    "rf_data_point = pd.DataFrame(columns=['time', 'daily precipitation'])\n",
    "rf_data_point['daily precipitation'] = precip_mon.sel(lat=lat_near, lon=lon_near)['daily precipitation'].values\n",
    "rf_data_point['time'] = precip_mon.sel(lat=lat_near, lon=lon_near)['time'].values\n",
    "#rf_data_point= rf_data_point[rf_data_point['daily precipitation']>=cutoff]\n",
    "df_spi_point_sort= rf_data_point.copy()\n",
    "df_spi_point_sort= df_spi_point_sort.sort_values(by='daily precipitation', ascending=True)\n",
    "#df_spi_point_sort= df_spi_point_sort[df_spi_point_sort['daily precipitation']>=cutoff]\n",
    "\n",
    "# Plot histogram of monthly precipitation\n",
    "pre_dist= rf_data_point['daily precipitation'].values\n",
    "#pre_dist[np.isnan(pre_dist)] = 0\n",
    "precipitation= pre_dist\n",
    "gamma_fit, beta, alpha, A = fit_gamma(precipitation, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91847dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import scipy.stats as scs\n",
    "from scipy.stats import gamma as gamma_scs\n",
    "import scipy.special\n",
    "\n",
    "def fit_gamma(X, cutoff):\n",
    "    \"\"\"\n",
    "    Fits a gamma distribution to a dataset X using the method described in the original paper.\n",
    "\n",
    "    Parameters:\n",
    "    X (numpy array): The dataset to fit the gamma distribution to.\n",
    "    cutoff (numeric): The minimum value in X to include in the fit.\n",
    "\n",
    "    Returns:\n",
    "    gamma (numpy array): The probability density function values of the fitted gamma distribution.\n",
    "    beta (numeric): The beta parameter of the fitted gamma distribution.\n",
    "    alpha (numeric): The alpha parameter of the fitted gamma distribution.\n",
    "    A (numeric): The A value used in the calculation of alpha and beta.\n",
    "    \"\"\"\n",
    "\n",
    "    # Sort the data and remove NaNs and values below the cutoff\n",
    "    X = np.sort(X)\n",
    "    Y = X.copy()\n",
    "    X = X[~np.isnan(X)]\n",
    "    X = X[X > cutoff]\n",
    "\n",
    "    # Calculate the values of alpha and beta\n",
    "    n = len(X)\n",
    "    A = np.log(np.mean(X)) - np.sum(np.log(X)) / n\n",
    "    alpha = 1 / (4 * A) * (1 + np.sqrt(1 + 4 * A / 3))\n",
    "    beta = np.mean(X) / alpha\n",
    "\n",
    "    # Calculate the probability density function values for each data point\n",
    "    n = len(Y)\n",
    "    gamma = np.zeros(n)\n",
    "    for i, x in enumerate(Y):\n",
    "        if x>0:\n",
    "            gamma[i] = 1 / (beta ** alpha * scipy.special.gamma(alpha)) * x ** (alpha - 1) * np.exp(-x / beta)\n",
    "        else:\n",
    "            gamma[i] = np.nan\n",
    "    cdf_dist = gamma_scs.cdf(np.sort(Y), a=alpha, loc=0, scale=beta)\n",
    "    q=len(Y[Y==0]) / len(Y) # Number of zeroes divided by number of total observations\n",
    "    print(q)\n",
    "    H = q + (1 - q) * cdf_dist\n",
    "    norm_ppf=spi_transform(H)\n",
    "    #norm_ppf = scs.norm.ppf(H) # same results as above but requires extra line below\n",
    "    #norm_ppf[np.isinf(norm_ppf)] = np.nan\n",
    "    \n",
    "    return gamma, cdf_dist, norm_ppf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb22a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import scipy.stats as scs\n",
    "from scipy.stats import fisk, pearson3, lognorm\n",
    "\n",
    "def fit_pearson3(X):\n",
    "    \"\"\"\n",
    "    Fits a log-logistic distribution to a dataset X using the method described in the original paper.\n",
    "\n",
    "    Parameters:\n",
    "    X (numpy array): The dataset to fit the log-logistic distribution to.\n",
    "    cutoff (numeric): The minimum value in X to include in the fit.\n",
    "\n",
    "    Returns:\n",
    "    gamma (numpy array): The probability density function values of the fitted log-logistic distribution.\n",
    "    alpha (numeric): The alpha parameter of the fitted log-logistic distribution.\n",
    "    beta (numeric): The beta parameter of the fitted log-logistic distribution.\n",
    "    A (numeric): The A value used in the calculation of alpha and beta.\n",
    "    \"\"\"\n",
    "\n",
    "    # Sort the data and remove NaNs and values below the cutoff\n",
    "    X = X[~np.isnan(X)]\n",
    "    X = np.sort(X)\n",
    "    #print(np.max(X))\n",
    "    #Y = X.copy()\n",
    "    \n",
    "    #X = X[X > cutoff]\n",
    "    # Calculate the values of moments\n",
    "    n = len(X)\n",
    "    w0=0\n",
    "    w1=0\n",
    "    w2=0\n",
    "    for i in range(n):\n",
    "        Fi=(i-0.35)/n\n",
    "        w0=w0+1/n*(1-Fi)**0*X[i]\n",
    "        w1=w1+1/n*(1-Fi)**1*X[i]\n",
    "        w2=w2+1/n*(1-Fi)**2*X[i]\n",
    "    # Calculate the values of alpha, beta and gamma\n",
    "    beta = (2*w1-w0)/(6*w1-w0-6*w2)\n",
    "    alpha = ((w0-2*w1)*beta)/(math.gamma(1+1/beta)*math.gamma(1-1/beta))\n",
    "    gamma = w0-alpha*(math.gamma(1+1/beta)*math.gamma(1-1/beta))\n",
    "    # Calculate the probability density function values for each data point\n",
    "    pdf_dist = np.zeros(n)\n",
    "    cdf_dist = np.zeros(n)\n",
    "    for i, x in enumerate(X):\n",
    "        pdf_dist[i] = (beta/alpha)*((x-gamma)/alpha)**(beta-1)*(1+((x-gamma)/alpha)**beta)**(-2)\n",
    "        cdf_dist[i] = (1+(alpha/(x-gamma))**beta)**(-1)\n",
    "    \n",
    "    norm_ppf=spi_transform(cdf_dist)\n",
    "\n",
    "    return pdf_dist, cdf_dist, norm_ppf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba3ea9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb4511c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c885c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from matplotlib.collections import LineCollection\n",
    "def plot_spi(df_spi_point_sort, option):\n",
    "    # get the SPI/SPEI values\n",
    "    spi_values = df_spi_point_sort[option]\n",
    "    # Plot SPI over time\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    y1positive=np.array(spi_values)>=0\n",
    "    y1negative = np.array(spi_values)<=0\n",
    "    plt.fill_between(df_spi_point_sort['time'], spi_values, y2=0,where=y1positive,\n",
    "    color='blue',alpha=0.5,interpolate=False)\n",
    "    plt.fill_between(df_spi_point_sort['time'], spi_values, y2=0,where=y1negative,\n",
    "    color='red',alpha=0.5,interpolate=False)\n",
    "    #colors = ['r' if spi < 0 else 'b' for spi in spi_values]\n",
    "    #plt.plot(df_spi_point_sort['time'], spi_values, label='SPI', color=colors)\n",
    "    plt.axhline(0, color='k', linestyle='--')  # add a horizontal line at y=0\n",
    "    plt.title(option+' Timeseries')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel(option)\n",
    "    plt.ylim([-4,4])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30172dcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1558b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from scipy.stats import invgauss, kstest, gamma, pearson3, genextreme, norm, expon, weibull_max, gumbel_l, invgamma, lognorm, rayleigh, skewnorm \n",
    "import scipy.stats as scs\n",
    "from scipy.stats import gamma as gamma_scs\n",
    "def ecdf(data):\n",
    "    # Compute ECDF\n",
    "    x = np.sort(data)\n",
    "    n = x.size\n",
    "    y = np.arange(1, n+1) / n\n",
    "    return(x,y)\n",
    "def plot_distributions(lat, lon, precip_mon, dist, n_bins, cutoff, option, f):\n",
    "    lat_near= find_nearest(precip_mon.lat.values, lat)\n",
    "    lon_near= find_nearest(precip_mon.lon.values, lon)\n",
    "    rf_data_point = pd.DataFrame(columns=['time', 'precipitation'])\n",
    "    rf_data_point['precipitation'] = precip_mon.sel(lat=lat_near, lon=lon_near)['precipitation'].values\n",
    "    rf_data_point['time'] = precip_mon.sel(lat=lat_near, lon=lon_near)['time'].values\n",
    "    #rf_data_point= rf_data_point[rf_data_point['daily precipitation']>=cutoff]\n",
    "    df_spi_point_sort= rf_data_point.copy()\n",
    "    df_spi_point_sort= df_spi_point_sort.sort_values(by='precipitation', ascending=True)\n",
    "    #df_spi_point_sort= df_spi_point_sort[df_spi_point_sort['daily precipitation']>cutoff]\n",
    "    pre_dist= rf_data_point['precipitation'].values\n",
    "    #pre_dist[np.isnan(pre_dist)] = 0\n",
    "    if option=='SPI': # results for SPI\n",
    "        # Plot histogram of monthly precipitation\n",
    "        precipitation= pre_dist[pre_dist>cutoff] # ignore dry days\n",
    "        #dates=dates[np.where(pre_dist>0)]\n",
    "        # Compute histogram of monthly precipitation and fit a gamma distribution\n",
    "        hist, bin_edges = np.histogram(precipitation, bins=n_bins, density=False)\n",
    "        bin_centers = (bin_edges[:-1] + bin_edges[1:])/2\n",
    "        total_count = len(precipitation)\n",
    "        bin_width = bin_edges[1] - bin_edges[0]\n",
    "        gamma_fit, cdf_dist, norm_ppf = fit_gamma(pre_dist, 0) \n",
    "        #gamma_fit2= gamma_fit[gamma_fit>0]\n",
    "        # Generate x values for gamma distribution PDF\n",
    "        #x = np.linspace(0, np.max(precipitation), len(df_spi_point_sort))\n",
    "        #x = np.linspace(0, np.max(precipitation), len(gamma_fit))\n",
    "        # Plot histogram of monthly precipitation and fitted gamma distribution\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "        ax.hist(precipitation, bins=n_bins, density=False, alpha=0.5, edgecolor='black')\n",
    "        ax.plot(np.sort(pre_dist), gamma_fit* total_count * bin_width, color='red', label='Fitted Gamma Distribution')\n",
    "        #ax.scatter(precipitation, np.zeros_like(precipitation) + 0.01, alpha=0.5, label='Empirical data')\n",
    "        #ax.plot(x, dist.pdf(x, *params)* total_count * bin_width, color='red', label='Fitted Distribution')\n",
    "        ax.set_title('Precipitation at point near Stockholm')\n",
    "        ax.set_xlabel('Precipitation (mm)')\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.set_xlim([0, np.max(precipitation)])\n",
    "        #ax.set_ylim([0, 300])\n",
    "        ax.legend()\n",
    "        fig.show()\n",
    "        # Compute cumulative distribution function for fitted gamma distribution\n",
    "        #cdf_dist = dist.cdf(x, *params)\n",
    "        ##cdf_dist = gamma.cdf(np.sort(pre_dist), a=alpha, loc=0, scale=beta)\n",
    "        #x = np.linspace(0, np.max(precipitation), len(df_spi_point_sort))\n",
    "        #x = np.linspace(0, np.max(precipitation), len(cdf_dist))\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "        x_emp, y_emp=ecdf(precipitation)\n",
    "        plt.plot(x_emp, y_emp, '.', color='black', label='Observed Points')\n",
    "        plt.plot(np.sort(pre_dist), np.sort(cdf_dist), color='red', label='Fitted Distribution')\n",
    "        plt.title('Cumulative Probability Function')\n",
    "        plt.xlabel('Precipitation (mm)')\n",
    "        plt.ylabel('Cumulative Probability')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        # Plot cumulative distribution function for fitted gamma distribution and SPI values\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "        plt.plot(norm_ppf, cdf_dist, color='red', label='Transformed Distribution')\n",
    "        #plt.plot(spi, np.arange(len(spi))/float(len(spi)), '.', color='black', label='Observed Points')\n",
    "        plt.title('Standardized Precipitation Index')\n",
    "        plt.xlabel('Standardized Precipitation Index')\n",
    "        plt.ylabel('Cumulative Probability')\n",
    "        plt.xlim([-4,4])\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        df_spi_point_sort['SPI']=norm_ppf\n",
    "        df_spi_point_sort=df_spi_point_sort.sort_values(by='time', ascending=True)\n",
    "    elif option=='SPEI': # results for SPEI\n",
    "        pet_daily=[]\n",
    "        tmax_daily=tmax_ds.sel(lat=lat_near, lon=lon_near)['maximum daily air temperature'].values\n",
    "        tmin_daily=tmin_ds.sel(lat=lat_near, lon=lon_near)['maximum daily air temperature'].values\n",
    "        time_daily=tmax_ds.sel(lat=lat_near, lon=lon_near)['time'].values\n",
    "        for i in range(len(time_daily)):\n",
    "            Tmax=tmax_daily[i]\n",
    "            Tmin=tmin_daily[i]\n",
    "            date=time_daily[i]\n",
    "            #print(type(date))\n",
    "            pet_daily.append(hargreaves(Tmin, Tmax, date, lat_near))\n",
    "        pet_df=pd.DataFrame(columns=('time','pet'))\n",
    "        pet_df['time']=pd.to_datetime(time_daily)\n",
    "        pet_df['pet']=pet_daily\n",
    "        pet_mon=pet_df.set_index('time').rolling(f, min_periods=f).sum().resample('1M').first()\n",
    "        #pet_mon=pet_mon.fillna(0)\n",
    "        rf_data_point['pre_excess']= rf_data_point['precipitation'].values-pet_mon['pet'].values\n",
    "        #df_spi_point_sort= df_spi_point_sort.sort_values(by='pre_excess', ascending=True)\n",
    "        # Plot histogram of monthly precipitation excess\n",
    "        pre_dist= rf_data_point['pre_excess'].values\n",
    "        #print(pet_mon['pet'])\n",
    "        pre_dist = pre_dist[~np.isnan(pre_dist)]\n",
    "        pdf_dist, cdf_dist, norm_ppf = fit_pearson3(pre_dist) \n",
    "        #pre_dist2 = pre_dist[~np.isnan(pre_dist)]\n",
    "        # Plot histogram of monthly precipitation excess\n",
    "        # Compute histogram of monthly precipitation and fit a gamma distribution\n",
    "        #norm_ppf=norm_ppf[~np.isnan(norm_ppf)]\n",
    "        hist, bin_edges = np.histogram(pre_dist, bins=n_bins, density=False)\n",
    "        bin_centers = (bin_edges[:-1] + bin_edges[1:])/2\n",
    "        total_count = len(pre_dist)\n",
    "        bin_width = bin_edges[1] - bin_edges[0]\n",
    "        # Generate x values for gamma distribution PDF\n",
    "        # Plot histogram of monthly precipitation and fitted gamma distribution\n",
    "        nan_array2 = np.empty(len(pdf_dist) - len(pre_dist))\n",
    "        nan_array2[:] = np.nan\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "        ax.hist(pre_dist, bins=n_bins, density=False, alpha=0.5, edgecolor='black')\n",
    "        ax.plot(np.sort(pre_dist), pdf_dist* total_count * bin_width, color='red', label='Fitted Log-logistic distribution')\n",
    "        ax.set_title('Precipitation excess at point near Stockholm')\n",
    "        ax.set_xlabel('Precipitation excess (mm)')\n",
    "        ax.set_ylabel('Frequency')\n",
    "        #ax.set_xlim([0, np.max(pre_dist)])\n",
    "        #ax.set_ylim([0, 300])\n",
    "        ax.legend()\n",
    "        fig.show()\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "        x_emp, y_emp=ecdf(pre_dist)\n",
    "        plt.plot(x_emp, y_emp, '.', color='black', label='Observed Points')\n",
    "        #pre_dist = np.concatenate((nan_array, pre_dist))\n",
    "        plt.plot(np.sort(pre_dist), np.sort(cdf_dist), color='red', label='Fitted Distribution')\n",
    "        plt.title('Cumulative Probability Function')\n",
    "        plt.xlabel('Precipitation Excess (mm)')\n",
    "        plt.ylabel('Cumulative Probability')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        # Plot cumulative distribution function for fitted gamma distribution and SPI values\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "        plt.plot(norm_ppf, cdf_dist, color='red', label='Transformed Distribution')\n",
    "        #plt.plot(spi, np.arange(len(spi))/float(len(spi)), '.', color='black', label='Observed Points')\n",
    "        plt.title('Standardized Precipitation Evapotranspiration Index')\n",
    "        plt.xlabel('SPEI')\n",
    "        plt.ylabel('Cumulative Probability')\n",
    "        plt.xlim([-4,4])\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        nan_array = np.empty(len(df_spi_point_sort.index) - len(norm_ppf))\n",
    "        nan_array[:] = np.nan\n",
    "        norm_ppf = np.concatenate((nan_array, norm_ppf))\n",
    "        #pdf_fit = np.concatenate((nan_array, pdf_fit))\n",
    "        #cdf_dist = np.concatenate((nan_array, cdf_dist))\n",
    "        df_spi_point_sort['SPEI']=norm_ppf\n",
    "        df_spi_point_sort=df_spi_point_sort.sort_values(by='time', ascending=True)\n",
    "    # Plot SPI over time\n",
    "    plot_spi(df_spi_point_sort, option)\n",
    "    return df_spi_point_sort\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9569bda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Polygon\n",
    "from cartopy.io import shapereader\n",
    "import cartopy.io.img_tiles as cimgt\n",
    "import cartopy.crs as ccrs\n",
    "import geopandas\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from scipy.interpolate import griddata\n",
    "import matplotlib.colors as colors\n",
    "from matplotlib.colors import LinearSegmentedColormap, BoundaryNorm\n",
    "import matplotlib.ticker as ticker\n",
    "plt.rcdefaults()\n",
    "resolution = '50m'\n",
    "category = 'cultural'\n",
    "name = 'admin_0_countries'\n",
    "\n",
    "shpfilename = shapereader.natural_earth(resolution, category, name)\n",
    "\n",
    "df = gpd.read_file(shpfilename)\n",
    "\n",
    "def rect_from_bound(xmin, xmax, ymin, ymax):\n",
    "    \"\"\"Returns list of (x,y)'s for a rectangle\"\"\"\n",
    "    xs = [xmax, xmin, xmin, xmax, xmax]\n",
    "    ys = [ymax, ymax, ymin, ymin, ymax]\n",
    "    return [(x, y) for x, y in zip(xs, ys)]\n",
    "\n",
    "def plot_map(interp_temps, title, label, lower_bound, upper_bound, vcenter, option, cmap_type, save, filename):\n",
    "    # request data for use by geopandas\n",
    "    resolution = '10m'\n",
    "    category = 'cultural'\n",
    "    name = 'admin_0_countries'\n",
    "    data=interp_temps.copy()\n",
    "    shpfilename = shapereader.natural_earth(resolution, category, name)\n",
    "    df = geopandas.read_file(shpfilename)\n",
    "\n",
    "    # get geometry of a country\n",
    "    poly = [df.loc[df['ADMIN'] == 'Sweden']['geometry'].values[0]]\n",
    "\n",
    "    stamen_terrain = cimgt.Stamen('terrain-background')\n",
    "    # define the colormap\n",
    "    if cmap_type=='SPI_cmap':\n",
    "        cmap = colors.ListedColormap(['red', 'orange', 'yellow', 'white', 'lavender', 'plum', 'purple'])\n",
    "        #define the boundaries of the SPI values for each color in the colormap\n",
    "        #bounds = [-np.inf, -2.0, -1.5, -1.0, 1.0, 1.5, 2.0, np.inf]\n",
    "        # create a norm object to map SPI values to colors\n",
    "        #norm = colors.BoundaryNorm(bounds, cmap.N)\n",
    "    elif cmap_type=='inverted':\n",
    "        cmap='coolwarm_r' #'RdBu'\n",
    "    else:\n",
    "        cmap='coolwarm' #'RdBu_r'\n",
    "\n",
    "    # projections that involved\n",
    "    st_proj = ccrs.PlateCarree()  #projection used by Stamen images\n",
    "    ll_proj = ccrs.PlateCarree()  #CRS for raw long/lat\n",
    "\n",
    "    # create fig and axes using intended projection\n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    ax = fig.add_subplot(1, 1, 1, projection=st_proj)\n",
    "    ax.add_geometries(poly, crs=ll_proj, facecolor='none', edgecolor='black')\n",
    "\n",
    "    pad1 = .1  #padding, degrees unit\n",
    "    exts = [poly[0].bounds[0] - pad1, poly[0].bounds[2] + pad1, poly[0].bounds[1] - pad1, poly[0].bounds[3] + pad1];\n",
    "    ax.set_extent(exts, crs=ll_proj)\n",
    "\n",
    "    # make a mask polygon by polygon's difference operation\n",
    "    # base polygon is a rectangle, another polygon is simplified switzerland\n",
    "    msk = Polygon(rect_from_bound(*exts)).difference( poly[0].simplify(0.01) )\n",
    "    msk_stm  = st_proj.project_geometry (msk, ll_proj)  # project geometry to the projection used by stamen\n",
    "\n",
    "    # get and plot Stamen images\n",
    "    #ax.add_image(stamen_terrain, 8) # this requests image, and plot\n",
    "\n",
    "    # plot the mask using semi-transparency (alpha=0.65) on the masked-out portion\n",
    "    ax.add_geometries( msk_stm, st_proj, zorder=12, facecolor='white', edgecolor='k', alpha=1)\n",
    "\n",
    "    #gl=ax.gridlines(draw_labels=True)\n",
    "    #gl.xlabels_top = False\n",
    "    #gl.ylabels_left = False\n",
    "    \n",
    "\n",
    "    if option=='imshow':\n",
    "        data[data>upper_bound]=upper_bound\n",
    "        data[data<lower_bound]=lower_bound\n",
    "        if cmap_type=='SPI_cmap':\n",
    "            bounds=[-3.0, -2.0, -1.5, -1.0, 1.0, 1.5, 2.0, 3.0]\n",
    "            norm = colors.BoundaryNorm(bounds, cmap.N)\n",
    "            im = ax.imshow(data, origin='lower', norm=norm,\n",
    "                extent=[lon_grid.min(), lon_grid.max(), lat_grid.min(), lat_grid.max()],\n",
    "                cmap=cmap, zorder=11)\n",
    "        else:\n",
    "            # Create bounds vector from 10th to 90th percentile with median in the center\n",
    "            # Calculate the distance between center, lower and upper bounds\n",
    "            lb=lower_bound\n",
    "            ub=upper_bound\n",
    "            dist = vcenter - lb\n",
    "            dist2 = ub - vcenter\n",
    "            # Create bounds vector from 10th to 90th percentile with median closer to p10\n",
    "            bounds = [lb, lb + 0.25*dist , lb + 0.4*dist, lb + 0.5*dist, lb + 0.7*dist, lb + 0.8*dist, lb + 0.9*dist, lb + dist, vcenter + 0.25*dist2, vcenter + 0.5*dist2, vcenter + 0.75*dist2, vcenter + 0.8*dist2, vcenter + 0.9*dist2, ub]\n",
    "            im = ax.imshow(data, origin='lower', #norm=BoundaryNorm(bounds, ncolors=256),\n",
    "                vmin= lb, vmax=ub,\n",
    "                extent=[lon_grid.min(), lon_grid.max(), lat_grid.min(), lat_grid.max()],\n",
    "                cmap=cmap, zorder=11)\n",
    "    else:\n",
    "        if cmap_type=='SPI_cmap':\n",
    "            bounds=[-3.0, -2.0, -1.5, -1.0, 1.0, 1.5, 2.0, 3.0]\n",
    "        else:\n",
    "            bounds= np.linspace(lower_bound, upper_bound, num=12)\n",
    "        data[data>upper_bound]=upper_bound\n",
    "        data[data<lower_bound]=lower_bound\n",
    "        # Interpolate missing values\n",
    "        data_interp = pd.DataFrame(data).interpolate(method='linear', axis=0).values\n",
    "        im = ax.contourf(lon_grid, lat_grid, data_interp, levels=bounds,\n",
    "            extent=[lon_grid.min(), lon_grid.max(), lat_grid.min(), lat_grid.max()],\n",
    "            cmap=cmap, vmin=lower_bound, vmax=upper_bound, zorder=11)\n",
    "\n",
    "    # add title\n",
    "    plt.title(title, fontsize=18)\n",
    "    # add a colorbar\n",
    "    cbar_ax = fig.add_axes([0.188, -0.04, 0.623, 0.05]) # [left, bottom, width, height]\n",
    "    num_ticks = 7  # Adjust this value as needed\n",
    "    tick_locator = ticker.MaxNLocator(nbins=num_ticks)\n",
    "    tick_formatter = ticker.ScalarFormatter(useMathText=True)\n",
    "    cbar = plt.colorbar(im, cax=cbar_ax, orientation='horizontal', shrink=1.2,\n",
    "                        ticks=tick_locator, format=tick_formatter, extend='both')\n",
    "    #plt.subplots_adjust(top=0.95, bottom=0.15, left=0.0, right=1, hspace=0.2, wspace=0.2)\n",
    "    cbar.ax.tick_params(labelsize=12)\n",
    "    cbar.set_label(label, fontsize=14)\n",
    "    if save=='yes':\n",
    "        fig.savefig('figures/my_figure.png', dpi=300, bbox_inches='tight')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdba1d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "spi_ds = xr.open_dataset('SPEI12.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963d9932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the month and year of interest\n",
    "target_month = 8\n",
    "target_year = 1992\n",
    "spi_time = spi_ds['time']\n",
    "delta_time = abs((spi_time.dt.month - target_month) + (spi_time.dt.year - target_year) * 12)\n",
    "t = delta_time.argmin()\n",
    "#t = 309\n",
    "month = spi_ds.time.values[t].astype('datetime64[M]').astype(int) % 12 + 1\n",
    "year = spi_ds.time.values[t].astype('datetime64[Y]').astype(int) + 1970\n",
    "#def plot_map(interp_temps, title, label, lower_bound, upper_bound, vcenter, option, cmap_type, save, filename):\n",
    "plot_map(spi_ds.SPEI.values[t,:], 'SPEI12', str(month)+'-'+str(year), -3, 3, 0, 'contourf', 'SPI_cmap', 'no', 'noname')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c16063",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e33481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_droughts(time, spi_point):\n",
    "    # initialize output arrays\n",
    "    dry_months = np.zeros_like(spi_point)\n",
    "    duration = np.zeros_like(spi_point)\n",
    "    intensity = np.zeros_like(spi_point)\n",
    "    frequency = np.zeros_like(spi_point)\n",
    "    starts = np.zeros_like(spi_point)\n",
    "    # find droughts\n",
    "    drought_start = -1\n",
    "    num_droughts = 0\n",
    "    num_extreme_droughts = 0 # new counter for extreme droughts\n",
    "    current_year = int(str(time[0].astype('M8[Y]')).split('-')[0])\n",
    "    for i, (spi, dt) in enumerate(zip(spi_point, time)):\n",
    "        year = int(str(dt.astype('M8[Y]')).split('-')[0])\n",
    "        if year > current_year:\n",
    "            current_year = year\n",
    "            num_droughts = 0\n",
    "            num_extreme_droughts = 0 # reset counters at the start of each new year\n",
    "\n",
    "        #if spi < 0: # old drought definition\n",
    "        if spi <=-1:\n",
    "            if drought_start == -1:\n",
    "                drought_start = i\n",
    "            dry_months[i] = 1\n",
    "        else:\n",
    "            if drought_start != -1:\n",
    "                drought_end = i - 1\n",
    "                drought_duration = drought_end - drought_start + 1\n",
    "                duration[drought_start] = drought_duration\n",
    "                starts[drought_start] = 1\n",
    "                drought_intensity = np.sum(spi_point[drought_start:drought_end+1])\n",
    "                intensity[drought_start] = drought_intensity\n",
    "                drought_peak = np.min(spi_point[drought_start:drought_end+1])\n",
    "                if drought_peak<=0: # # count all droughts\n",
    "                    frequency[drought_start] = num_extreme_droughts + 1 # increment the frequency counter\n",
    "                    num_extreme_droughts += 1 # increment the counter for extreme droughts only\n",
    "                num_droughts += 1 # always increment the total drought counter\n",
    "                drought_start = -1\n",
    "\n",
    "    # handle last drought if it exists\n",
    "    if drought_start != -1:\n",
    "        drought_end = len(spi_point) - 1\n",
    "        drought_duration = drought_end - drought_start + 1\n",
    "        duration[drought_start] = drought_duration\n",
    "        starts[drought_start] = 1\n",
    "        drought_intensity = np.sum(spi_point[drought_start:drought_end+1])\n",
    "        intensity[drought_start] = drought_intensity\n",
    "        drought_peak = np.min(spi_point[drought_start:drought_end+1])\n",
    "        if drought_peak<=0: # count all droughts\n",
    "            frequency[drought_start] = num_extreme_droughts + 1 # increment the frequency counter\n",
    "            num_extreme_droughts += 1 # increment the counter for extreme droughts only\n",
    "\n",
    "    return duration, intensity, dry_months, frequency, starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd69cfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70cfea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa9ecbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Compute drought intensity, duration and frequency at all points and save results\n",
    "spi_ds = xr.open_dataset('SPEI12.nc')\n",
    "lat_grid = np.linspace(55.3376, 69.1763, 100) \n",
    "lon_grid = np.linspace(11, 24.113, 100)\n",
    "\n",
    "# Create empty dataset with desired dimensions and coordinates\n",
    "ds = xr.Dataset(\n",
    "    {\n",
    "        'dr_durs': (['time', 'lat', 'lon'], np.zeros((len(spi_ds.time.values), 100, 100))),\n",
    "        'dr_ints': (['time', 'lat', 'lon'], np.zeros((len(spi_ds.time.values), 100, 100))),\n",
    "        'dr_mons': (['time', 'lat', 'lon'], np.zeros((len(spi_ds.time.values), 100, 100))),\n",
    "        'dr_freq': (['time', 'lat', 'lon'], np.zeros((len(spi_ds.time.values), 100, 100))),\n",
    "        'dr_star': (['time', 'lat', 'lon'], np.zeros((len(spi_ds.time.values), 100, 100))),\n",
    "    },\n",
    "    coords={\n",
    "        'time': spi_ds.time.values,\n",
    "        'lat': lat_grid,\n",
    "        'lon': lon_grid,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Fill in dataset with drought data\n",
    "for i in range(len(lat_grid)):\n",
    "    for j in range(len(lon_grid)):\n",
    "        print(i,j)\n",
    "        time = spi_ds.sel(lat=lat_grid[i], lon=lon_grid[j])['time'].values\n",
    "        spi_values = spi_ds.sel(lat=lat_grid[i], lon=lon_grid[j])['SPEI'].values\n",
    "        duration, intensity, dry_months, frequency, starts = find_droughts(time, spi_values)\n",
    "        ds['dr_durs'][:,i,j] = duration\n",
    "        ds['dr_ints'][:,i,j] = intensity\n",
    "        ds['dr_mons'][:,i,j] = dry_months\n",
    "        ds['dr_freq'][:,i,j] = frequency\n",
    "        ds['dr_star'][:,i,j] = starts\n",
    "\n",
    "# Save dataset as netCDF file\n",
    "ds.to_netcdf('drought_data_SPEI12.nc')\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b74d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "dr_ds = xr.open_dataset('drought_data_SPEI12.nc')\n",
    "dr_ds = dr_ds.sel(time=slice('1923-01-01', None)) # Use for SPEI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28527b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#np.nanmax(dr_ds.dr_durs.values)\n",
    "#max_dur = np.nanmax(dr_ds.dr_durs.values, axis=0) # maximum drought durations\n",
    "#max_int = np.nanmin(dr_ds.dr_ints.values, axis=0) # strongest drought intensities\n",
    "#tot_num = dr_ds.dr_nums.values\n",
    "# define a function to calculate percentiles along the time dimension\n",
    "def calc_percentiles(data):\n",
    "    return np.nanpercentile(data, [10, 50, 90], axis=0)\n",
    "# convert xarray data to a numpy array\n",
    "#dr_peak_np = spi_ds.SPEI.values\n",
    "#dr_peak_np[dr_peak_np==0]=np.nan\n",
    "#dr_durs_np = dr_ds.dr_durs.values\n",
    "dr_durs_np = dr_ds_decade.dr_durs.values # changed to total duration of droughts per decade\n",
    "#dr_durs_np[dr_durs_np==0]=np.nan\n",
    "dr_ints_np = dr_ds_decade.dr_ints.values\n",
    "#dr_ints_np[dr_ints_np==0]=np.nan\n",
    "#dr_freq_np = dr_ds_yearly.dr_freq.values # number of droughts per year\n",
    "#dr_freq_np = dr_ds_yearly.dr_mons.values # number of dry months per year\n",
    "#dr_freq_np = dr_ds_decade.dr_mons.values # number of dry months per decade\n",
    "#dr_freq_np = dr_ds_yearly.dr_star.values # number of droughts per year\n",
    "#dr_freq_np = dr_ds_decade2.dr_freq.values # number of droughts per decade\n",
    "dr_freq_np = dr_ds_decade.dr_star.values # number of droughts per decade\n",
    "#dr_freq_np[dr_freq_np==0]=np.nan\n",
    "#dr_freq_np = np.nan_to_num(dr_freq_np, nan=0)\n",
    "# calculate percentiles using the function and numpy array\n",
    "#peak_perc_np = calc_percentiles(dr_peak_np)\n",
    "dur_perc_np = calc_percentiles(dr_durs_np)\n",
    "int_perc_np = calc_percentiles(dr_ints_np)\n",
    "freq_perc_np = calc_percentiles(dr_freq_np)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840f2b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e2f1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot strongest intensity\n",
    "plt.rcdefaults()\n",
    "#plot_data=np.nanmin(dr_ints_np, axis=0)\n",
    "plot_data=int_perc_np[0]*(-1) # absolute values\n",
    "lb=int(np.percentile(plot_data[plot_data>0], 0.01))\n",
    "ub=int(np.percentile(plot_data[plot_data>0], 99.99))\n",
    "vcenter=int(np.percentile(plot_data[plot_data>0], 50))\n",
    "ub=14\n",
    "lb=6\n",
    "#vcenter=-300\n",
    "plot_map(plot_data, 'Drought Intensity Hotspots', 'Top decile of annual cumulative drought intensity', lb, ub, vcenter, 'imshow', 'normal','no','no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd1aa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd029ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot longest duration\n",
    "#plot_data=np.nanmax(dr_durs_np, axis=0)/12\n",
    "plot_data=dur_perc_np[2]\n",
    "lb=int(np.percentile(plot_data[plot_data>0], 5))\n",
    "ub=int(np.percentile(plot_data[plot_data>0], 95))\n",
    "#vcenter=int(np.percentile(plot_data[plot_data>0], 50))\n",
    "#ub=9\n",
    "#lb=4\n",
    "#vcenter=5\n",
    "plot_map(plot_data, 'Drought Duration Hotspots', 'Top decile of decadal cumulative drought duration (months)', lb, ub, vcenter, 'imshow', 'normal','no','no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b469e948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset Matplotlib settings to defaults\n",
    "#plt.rcParams.update(plt.rcParamsDefault)\n",
    "plot_data=freq_perc_np[2] # 90th percentile of droughts per decade or year depending on previous cell\n",
    "lb=int(np.percentile(plot_data[plot_data>0], 5))\n",
    "ub=int(np.percentile(plot_data[plot_data>0], 95))\n",
    "vcenter=int(np.percentile(plot_data[plot_data>0], 50))\n",
    "#ub=10\n",
    "#lb=4\n",
    "#vcenter=2\n",
    "#plot_map(plot_data, '90th Percentile Dry Month Count per Decade', lb, ub, vcenter, 'imshow', 'normal','no','no')\n",
    "plot_map(plot_data, 'Drought Frequency Hotspots', 'Top decile of decadal count of droughts', lb, ub, vcenter, 'imshow', 'normal','no','no')\n",
    "#plot_map(plot_data, 'Drought Frequency (Dry Months per Decade)', '90th Percentile Values', lb, ub, vcenter, 'imshow', 'normal','no','no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e703cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "ds=dr_ds_yearly\n",
    "# Create a function to plot scatter plots\n",
    "def plot_scatter(x, y, xlabel, ylabel, title):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(x=x, y=y)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Create the scatter plots\n",
    "plot_scatter(ds['dr_durs'].values.flatten(), ds['dr_freq'].values.flatten(), \n",
    "             'Duration', 'Frequency', 'Duration vs Frequency')\n",
    "plot_scatter(ds['dr_ints'].values.flatten(), ds['dr_freq'].values.flatten(), \n",
    "             'Intensity', 'Frequency', 'Intensity vs Frequency')\n",
    "plot_scatter(ds['dr_ints'].values.flatten(), ds['dr_durs'].values.flatten(), \n",
    "             'Intensity', 'Duration', 'Intensity vs Duration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c866a161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61697807",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4321c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "def plot_reg(df, colx, coly, lbx, lby):\n",
    "    df=df.dropna()\n",
    "    # Calculate correlation coefficient\n",
    "    corr, _ = pearsonr(df[colx], df[coly])\n",
    "    # Create scatter plot with regression line and correlation coefficient\n",
    "    g=sns.jointplot(x=colx, y=coly, data=df, kind='reg',  line_kws={\"color\":\"red\"})\n",
    "    # Add text box with correlation coefficient\n",
    "    g.ax_joint.text(0.1, 0.9, f\"R = {corr:.2f}\", fontsize=12,\n",
    "                transform=g.ax_joint.transAxes,\n",
    "                bbox=dict(facecolor='white', edgecolor='black', boxstyle='round'))\n",
    "    # Set custom x and y axis labels\n",
    "    g.set_axis_labels(lbx, lby)\n",
    "    # Display the plot\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5ad5fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70de5968",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629a0e44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab4ba67",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssi_ds = xr.open_dataset('SSI12.nc')\n",
    "ssi_df=ssi_ds.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716f9741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr, linregress\n",
    "\n",
    "def calculate_trends(data_array, alpha=0.05):\n",
    "    timesteps, n_lat, n_lon = data_array.shape\n",
    "\n",
    "    x = np.arange(timesteps)\n",
    "    rate_of_change = np.empty((n_lat, n_lon))\n",
    "\n",
    "    for i in range(n_lat):\n",
    "        for j in range(n_lon):\n",
    "            y = data_array[:, i, j]\n",
    "            mask = ~np.isnan(y)\n",
    "            if mask.sum() > 1:\n",
    "                slope, intercept, r_value, p_value, std_err = linregress(x[mask], y[mask])\n",
    "                if p_value <= alpha:\n",
    "                    rate_of_change[i, j] = slope\n",
    "                else:\n",
    "                    rate_of_change[i, j] = np.nan\n",
    "            else:\n",
    "                rate_of_change[i, j] = np.nan\n",
    "\n",
    "    return rate_of_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6baf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_ds = xr.open_dataset('drought_data_SPEI12.nc')\n",
    "dr_ds = dr_ds.sel(time=slice('1923-01-01', None)) # Use for SPEI\n",
    "dr_ds_yearly =dr_ds.resample(time='1Y').sum('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becec0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_dur_rate=calculate_trends(dr_durs_np) # Total duration per decade\n",
    "plot_map(dr_dur_rate, 'Drought Duration Trends', 'Annual change of cumulative drought duration (months)', -0.1, 0.1, 0, 'imshow','coolwarm', 'no', 'no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab96c9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dr_freq_corr=calculate_trends(dr_ds.dr_freq) \n",
    "dr_freq_rate=calculate_trends(dr_freq_np)\n",
    "plot_map(dr_freq_rate, 'Drought Frequency Trends', 'Decadal change of drought count', -1.7, 1.7, 0, 'imshow','coolwarm', 'no', 'no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46647de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture --no-display\n",
    "dr_int_rate=calculate_trends(dr_ints_np*(-1)) # Total drought intensity per year, -1 for positive intensities\n",
    "plot_map(dr_int_rate, 'Drought Intensity Trends', 'Decadal change of cumulative drought intensity', -0.1, 0.1, 0, 'imshow','coolwarm', 'no', 'no')\n",
    "#plot_map(dr_int_corr, 'Drought Intensity Trends (R-value)', -0.5, 0.5, 0, 'imshow', 'normal','no','no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28cd713",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f9af00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8f4d12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
