{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74713058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code downloads precipitation or river discharge data from SMHI's OpenData API \n",
    "# Author: Marlon Passos\n",
    "# Adapted from Kristoffer BÃ¤ckman https://github.com/thebackman/SMHI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92782c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hydrological Observations (for river discharge)\n",
    "ADR_VERSION   = \"http://opendata-download-hydroobs.smhi.se/api/version/1.0.json\"\n",
    "ADR_PARAMETER = \"http://opendata-download-hydroobs.smhi.se/api/version/1.0/parameter/{parameter}.json\" \n",
    "ADR_STATION = \"http://opendata-download-hydroobs.smhi.se/api/version/1.0/parameter/{parameter}/station/{station}.json\"\n",
    "ADR_LATEST_MONTHS = \"http://opendata-download-hydroobs.smhi.se/api/version/1.0/parameter/{parameter}/station/{station}/period/latest-months/data.json\"\n",
    "ADR_CORRECTED = \"http://opendata-download-hydroobs.smhi.se/api/version/1.0/parameter/{parameter}/station/{station}/period/corrected-archive/data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99500952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meteorological Observations (for precipitation)\n",
    "ADR_VERSION   = \"http://opendata-download-metobs.smhi.se/api/version/1.0.json\"\n",
    "ADR_PARAMETER = \"http://opendata-download-metobs.smhi.se/api/version/1.0/parameter/{parameter}.json\" \n",
    "ADR_STATION = \"http://opendata-download-metobs.smhi.se/api/version/1.0/parameter/{parameter}/station/{station}.json\"\n",
    "ADR_LATEST_MONTHS = \"http://opendata-download-metobs.smhi.se/api/version/1.0/parameter/{parameter}/station/{station}/period/latest-months/data.json\"\n",
    "ADR_LATEST_DAY = \"http://opendata-download-metobs.smhi.se/api/version/1.0/parameter/{parameter}/station/{station}/period/latest-day/data.json\"\n",
    "ADR_CORRECTED = \"http://opendata-download-metobs.smhi.se/api/version/1.0/parameter/{parameter}/station/{station}/period/corrected-archive/data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b28fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import sys\n",
    "import logging\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# -- logging\n",
    "\n",
    "FORMAT = '%(asctime)s %(levelname)s: %(module)s: %(funcName)s(): %(message)s'\n",
    "logging.basicConfig(level=logging.DEBUG, format = FORMAT, filename = \"smhi.log\", filemode = \"w\")\n",
    "logging.getLogger(\"requests\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n",
    "\n",
    "# -- functions\n",
    "\n",
    "def write_json(json_obj, file_name = 'file.json'):\n",
    "    \"\"\" write a json file to wd/file.json\"\"\"\n",
    "    with open(file_name, 'w') as outfile:\n",
    "        json.dump(json_obj, outfile)\n",
    "\n",
    "def api_return_data(adr):\n",
    "    \"\"\" initate API call and return the JSON data \"\"\"\n",
    "    # initiate the call\n",
    "    req_obj = requests.get(adr)\n",
    "    # try to get the json data (exceptions will be catched later)\n",
    "    json_data = req_obj.json()\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1e742c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import sys\n",
    "import logging\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# -- logging\n",
    "\n",
    "FORMAT = '%(asctime)s %(levelname)s: %(module)s: %(funcName)s(): %(message)s'\n",
    "logging.basicConfig(level=logging.DEBUG, format = FORMAT, filename = \"smhi.log\", filemode = \"w\")\n",
    "logging.getLogger(\"requests\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n",
    "\n",
    "# -- functions\n",
    "\n",
    "def write_json(json_obj, file_name = 'file.json'):\n",
    "    \"\"\" write a json file to wd/file.json\"\"\"\n",
    "    with open(file_name, 'w') as outfile:\n",
    "        json.dump(json_obj, outfile)\n",
    "\n",
    "def api_return_data(adr):\n",
    "    \"\"\" initate API call and return the JSON data \"\"\"\n",
    "    # initiate the call\n",
    "    req_obj = requests.get(adr)\n",
    "    # try to get the json data (exceptions will be catched later)\n",
    "    json_data = req_obj.json()\n",
    "    return json_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553ab7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import api_endpoints\n",
    "#import helpers\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# -- functions\n",
    "\n",
    "def list_params():\n",
    "    \"\"\" Lists avaliable parameters \"\"\"\n",
    "\n",
    "    # -- API call\n",
    "\n",
    "    data_json = api_return_data(ADR_VERSION)\n",
    "\n",
    "    # -- print collected data\n",
    "\n",
    "    # subset and loop over all avaliable parameters\n",
    "    resource = data_json[\"resource\"]\n",
    "    # loop over the json entries and print each parameter that is avaliable\n",
    "\n",
    "    # store all param keys in a dict for later use (maybe)\n",
    "    params = []\n",
    "    for param in resource:\n",
    "        print(param[\"title\"] + \" | \" +  param[\"summary\"] + \" | \" + param[\"key\"] )\n",
    "        params.append(param[\"key\"])\n",
    "    # return params\n",
    "\n",
    "\n",
    "def list_stations(param):\n",
    "    \"\"\" list stations that have a certain wheather parameter \"\"\"\n",
    "\n",
    "    # -- API call\n",
    "    \n",
    "    # create the API adress\n",
    "    adr = ADR_PARAMETER\n",
    "    adr_full = adr.format(parameter = param)\n",
    "\n",
    "    # send request and get data\n",
    "    data1 = api_return_data(adr_full)\n",
    "    print(\"Parameter choosen: \" + data1[\"title\"])\n",
    "\n",
    "    # -- gather and wrangle the data about avaliable stations\n",
    "    \n",
    "    # take out an array with all the stations\n",
    "    stations = data1[\"station\"]\n",
    "    \n",
    "    # convert the JSON data to a pandas DF\n",
    "    df_raw = pd.DataFrame(stations)\n",
    "    \n",
    "    # limit the data frame\n",
    "    df_clean = df_raw[[\"name\", \"id\", \"latitude\", \"longitude\", \"active\", \"from\", \"to\", \"updated\", \"title\", \"key\"]]\n",
    "    \n",
    "    # rename columns to abide to python reserved keywords\n",
    "    df_clean = df_clean.rename(columns={\"from\": \"starting\", \"to\": \"ending\"})\n",
    "    \n",
    "    # fix the date and time variables into something readable\n",
    "    tmp1 = pd.to_datetime(df_clean[\"starting\"], unit = \"ms\")\n",
    "    tmp2 = pd.to_datetime(df_clean[\"ending\"], unit = \"ms\")\n",
    "    tmp3 = pd.to_datetime(df_clean[\"updated\"], unit = \"ms\")\n",
    "    df_clean = df_clean.assign(starting = tmp1)\n",
    "    df_clean = df_clean.assign(ending = tmp2)\n",
    "    df_clean = df_clean.assign(updated = tmp3)\n",
    "    \n",
    "    return(df_clean)\n",
    "\n",
    "\n",
    "def get_latest_months(param, station):\n",
    "    \"\"\"\n",
    "    get data for latest months via JSON download \"\"\"\n",
    "\n",
    "    # -- API call\n",
    "    \n",
    "    # create the API adress\n",
    "    adr = ADR_LATEST_MONTHS\n",
    "    adr_full = adr.format(parameter = param, station = station)\n",
    "    \n",
    "    # send request and get data\n",
    "    data1 = api_return_data(adr_full)\n",
    "    \n",
    "    # create a data frame from the JSON data\n",
    "    df = pd.DataFrame(data1[\"value\"])\n",
    "\n",
    "    # fix the timestamps\n",
    "    df = df.rename(columns={\"from\": \"starting\", \"to\": \"ending\"})\n",
    "    tmp1 = pd.to_datetime(df[\"starting\"], unit = \"ms\")\n",
    "    tmp2 = pd.to_datetime(df[\"ending\"], unit = \"ms\")\n",
    "    df = df.assign(starting = tmp1)\n",
    "    df = df.assign(ending = tmp2)\n",
    "\n",
    "    # convert value to float64\n",
    "    df[\"value\"] = df.value.astype(float)\n",
    "\n",
    "    # add the station id\n",
    "    df[\"station_id\"] = station\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_corrected(param, station):\n",
    "    \"\"\" get corrected archive via CSV download \"\"\"\n",
    "    # -- API call\n",
    "\n",
    "    # create the API adress\n",
    "    adr = ADR_CORRECTED\n",
    "    adr_full = adr.format(parameter = param, station = station)\n",
    "\n",
    "    # download the csv data\n",
    "    df = pd.read_csv(filepath_or_buffer= adr_full, skiprows= 7, delimiter=\";\")\n",
    "\n",
    "    # remove columns not needed and reorder to match latest months data\n",
    "    df_lim = df.iloc[:,[0, 1, 2, 5]]\n",
    "\n",
    "    # rename the columns\n",
    "    df_lim.columns = [\"date\", \"value\", \"quality\", \"ref\"]\n",
    "\n",
    "    # fix datetime columns\n",
    "    df_lim[\"date\"] = pd.to_datetime(df_lim[\"date\"])\n",
    "\n",
    "    # add the station id\n",
    "    df_lim[\"station_id\"] = station\n",
    "\n",
    "    return df_lim\n",
    "\n",
    "\n",
    "def get_stations(param, station_keys):\n",
    "    \"\"\"\n",
    "    gets both latest months and corrected archive for\n",
    "    a set of stations. Contains the try catch logic needed\n",
    "    if any of the calls fail\n",
    "    \"\"\"\n",
    "\n",
    "    # -- create the iterable\n",
    "\n",
    "    if isinstance(station_keys, tuple):\n",
    "        iterable = station_keys\n",
    "    elif isinstance(station_keys, pd.DataFrame):\n",
    "        iterable = station_keys[\"key\"]\n",
    "    \n",
    "    # -- Construct some holder structures for data frames\n",
    "\n",
    "    df_new = dict()\n",
    "    df_old = dict()\n",
    "    \n",
    "    # -- loop through set of stations\n",
    "    \n",
    "    # start loop over each station id and collect the data if avaliable\n",
    "    print(\">>> Start downloading each station\")\n",
    "    for station_id in iterable:\n",
    "        print(f\">>> Downloading {station_id}\")\n",
    "        #logging.info(f\"# -- Downloading station {station_id}\")\n",
    "        # get the latest months\n",
    "       # logging.info(f\"Downloading latest months for {station_id}\")\n",
    "       # try:\n",
    "            #df_new[station_id] = get_latest_months(param = param, station = station_id )\n",
    "           # logging.debug(f\"downloading latest months for {station_id} successful\")\n",
    "       # except: #json.decoder.JSONDecodeError:\n",
    "            #logging.error(f\"not possible to download latest months for {station_id}\")\n",
    "        # get the corrected archive\n",
    "        logging.info(f\"downloading corrected archive for {station_id}\")\n",
    "        try:\n",
    "            df_old[station_id] = get_corrected(param = param, station = station_id)\n",
    "            logging.debug(f\"downloaded corrected archive for {station_id} successful\")\n",
    "        except Exception as error:\n",
    "            logging.error(f\"not possible to download corrected archive for {station_id}\")\n",
    "    \n",
    "    # -- gather the data\n",
    "\n",
    "    # get the number of data frames in each dict\n",
    "    len_new = len(df_new)\n",
    "    len_old = len(df_old)\n",
    "\n",
    "    # Stack the latest months into one data frame for each station\n",
    "    if len_new > 0:\n",
    "        df_latest = pd.concat(df_new.values(), ignore_index=True)\n",
    "    else:\n",
    "        df_latest = None\n",
    "\n",
    "    # Stack the corrected archive into one data frame for each station\n",
    "    if len_old > 0:\n",
    "        df_corrected = pd.concat(df_old.values(), ignore_index=True)\n",
    "    else:\n",
    "        df_corrected = None\n",
    "\n",
    "    # return all data\n",
    "    print(\"Check smhi.log for data download details!\")\n",
    "    if df_latest is not None and df_corrected is not None :\n",
    "        logging.debug(\"both df_latest and df_corrected contains data\")\n",
    "        dictus = {\"df_latest\": df_latest, \"df_corrected\": df_corrected}\n",
    "    elif df_latest is not None:\n",
    "        logging.info(\"only df_latest contain data\")\n",
    "        dictus = {\"df_latest\": df_latest}\n",
    "    elif df_corrected is not None:\n",
    "        logging.info(\"only df_corrected contain data\")\n",
    "        dictus = {\"df_corrected\": df_corrected}\n",
    "    else:\n",
    "        logging.info(\"no data frame contains data\")\n",
    "        dictus = None\n",
    "    \n",
    "    # -- shutdown logging\n",
    "\n",
    "    logging.shutdown()\n",
    "    \n",
    "    return(dictus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3813a203",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -- import the libs and some reload \n",
    "\n",
    "import pandas as pd\n",
    "#import smhi as smhi\n",
    "#import importlib\n",
    "#importlib.reload(smhi)\n",
    "\n",
    "# -- listing the parameters that are avaliable\n",
    "\n",
    "params=list_params()\n",
    "\n",
    "# -- for one parameter, see what stations have it and in what timeframe, lon lat area etc.\n",
    "\n",
    "# all stations \n",
    "df_stations = list_stations(param = 1)\n",
    "station = (2506, 2372)\n",
    "\n",
    "# download for station(s)\n",
    "dict_df = get_stations(param = 1, station_keys = station)\n",
    "\n",
    "# access the data\n",
    "#dict_df[\"df_latest\"]\n",
    "dict_df[\"df_corrected\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d2c4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations.sort_values('starting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2712c0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df_stations['duration']=df_stations['ending']-df_stations['starting']\n",
    "df_stations['duration']=df_stations['duration']/ np.timedelta64(1, 'Y') # Convert from days to years\n",
    "df_stations.sort_values(by='duration',ascending=False).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64be4420",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_stations.to_excel('discharge_stations.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd351deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_stations= pd.read_excel('discharge_stations.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7132f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations= pd.read_excel('rain_stations.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcd9c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f39fbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert strings to datetime\n",
    "df_stations['starting'] = pd.to_datetime(df_stations['starting'], errors='coerce')\n",
    "\n",
    "# Filter your DataFrame using datetime comparison\n",
    "start_date_str = '1922-01-01 00:00:00'\n",
    "start_date = pd.to_datetime(start_date_str)\n",
    "\n",
    "filtered_df = df_stations[df_stations['starting'] > start_date]\n",
    "\n",
    "# Now you can use the filtered_df for plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a18fdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import cartopy.io.img_tiles as cimgt\n",
    "from cartopy.io.shapereader import Reader\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Sort your DataFrame\n",
    "df_stations = df_stations.sort_values(by='duration')\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "# Creates the map\n",
    "sw_map = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "# To add county lines\n",
    "mypath = r'C:\\Users\\MarlonVieiraPassos\\OneDrive - SEI\\Documents\\HydroHazards\\Codes\\OSM\\halmstad\\data\\shapefiles\\\\'\n",
    "data = Reader(mypath + 'se_10km.shp')\n",
    "sw_border = list(data.geometries())\n",
    "sw_border2 = cfeature.ShapelyFeature(sw_border, ccrs.PlateCarree())\n",
    "sw_map.add_feature(sw_border2, facecolor='none', edgecolor='gray')\n",
    "\n",
    "request = cimgt.MapboxTiles(map_id='light-v10',access_token=\"pk.eyJ1IjoibXZwYXNzb3MiLCJhIjoiY2t6em4xcnpwMGJ2bDNicDYyemM0bWU4YyJ9.fR55BgEafulrz4yLfY2oRA\")\n",
    "sw_map.add_image(request, 5, cmap='gray', interpolation='spline36', regrid_shape=2000)\n",
    "\n",
    "sw_map.xaxis.set_visible(True)\n",
    "sw_map.yaxis.set_visible(True)\n",
    "\n",
    "# Plots the data onto map\n",
    "cmap = matplotlib.cm.coolwarm\n",
    "norm = matplotlib.colors.Normalize(vmin=0, vmax=100)\n",
    "\n",
    "plt.scatter(df_stations['longitude'], df_stations['latitude'], alpha=0.8,\n",
    "            s=df_stations['duration'] / 2,\n",
    "            c=df_stations['duration'],\n",
    "            edgecolors='black',\n",
    "            cmap=cmap,\n",
    "            norm=norm,  # Apply the normalization to the scatter plot\n",
    "            transform=ccrs.PlateCarree())\n",
    "\n",
    "# Colorbar\n",
    "cbar = plt.colorbar(extend='max')\n",
    "cbar.set_label('Data Availability (years)', fontsize=16)\n",
    "cbar.ax.tick_params(labelsize=14)\n",
    "# Plot labels\n",
    "plt.ylabel(\"Latitude\", fontsize=14)\n",
    "plt.xlabel(\"Longitude\", fontsize=14)\n",
    "plt.title('Precipitation Observation Stations (1922-2021)', fontsize=16)\n",
    "\n",
    "plt.savefig('precipitation_stations_smhi2.png', format=\"png\", dpi=120)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a85542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import cartopy.io.img_tiles as cimgt\n",
    "from cartopy.io.shapereader import Reader\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Sort your DataFrame\n",
    "df_stations = df_stations.sort_values(by='duration')\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "# Creates the map\n",
    "sw_map = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "# To add county lines\n",
    "mypath = r'C:\\Users\\MarlonVieiraPassos\\OneDrive - SEI\\Documents\\HydroHazards\\Codes\\OSM\\halmstad\\data\\shapefiles\\\\'\n",
    "data = Reader(mypath + 'se_10km.shp')\n",
    "sw_border = list(data.geometries())\n",
    "sw_border2 = cfeature.ShapelyFeature(sw_border, ccrs.PlateCarree())\n",
    "sw_map.add_feature(sw_border2, facecolor='none', edgecolor='gray')\n",
    "\n",
    "request = cimgt.MapboxTiles(map_id='light-v10',access_token=\"pk.eyJ1IjoibXZwYXNzb3MiLCJhIjoiY2t6em4xcnpwMGJ2bDNicDYyemM0bWU4YyJ9.fR55BgEafulrz4yLfY2oRA\")\n",
    "sw_map.add_image(request, 5, cmap='gray', interpolation='spline36', regrid_shape=2000)\n",
    "\n",
    "sw_map.xaxis.set_visible(True)\n",
    "sw_map.yaxis.set_visible(True)\n",
    "\n",
    "# Plots the data onto map\n",
    "cmap = matplotlib.cm.coolwarm\n",
    "norm = matplotlib.colors.Normalize(vmin=0, vmax=100)\n",
    "\n",
    "plt.scatter(df_stations['longitude'], df_stations['latitude'], alpha=0.8,\n",
    "            s=df_stations['duration'] / 2,\n",
    "            c=df_stations['duration'],\n",
    "            edgecolors='black',\n",
    "            cmap=cmap,\n",
    "            norm=norm,  # Apply the normalization to the scatter plot\n",
    "            transform=ccrs.PlateCarree())\n",
    "\n",
    "# Colorbar\n",
    "cbar = plt.colorbar(extend='max')\n",
    "cbar.set_label('Data Availability (years)', fontsize=16)\n",
    "cbar.ax.tick_params(labelsize=14)\n",
    "# Plot labels\n",
    "plt.ylabel(\"Latitude\", fontsize=14)\n",
    "plt.xlabel(\"Longitude\", fontsize=14)\n",
    "plt.title('Modelled Discharge Locations (1922-2021)', fontsize=16)\n",
    "\n",
    "plt.savefig('discharge_stations_smhi2.png', format=\"png\", dpi=120)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534aad96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4e1faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data for all stations (monthly/daily rainfall)\n",
    "pnumber = 1 # 23: monthly rainfall, 5: daily rainfall\n",
    "df_all=pd.DataFrame()\n",
    "id_list=df_stations['id'].unique()\n",
    "for station_id in id_list:\n",
    "    try:\n",
    "        sid = (station_id,)\n",
    "        dict_df = get_stations(param = pnumber, station_keys = sid) \n",
    "        df_all=df_all.append(dict_df[\"df_corrected\"])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e28c3d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
