{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81477698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook exemplifies generating maps of hotspots and trends based on the gridded Daily Flood Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb72303",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Polygon\n",
    "from cartopy.io import shapereader\n",
    "import cartopy.io.img_tiles as cimgt\n",
    "import cartopy.crs as ccrs\n",
    "import geopandas\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from scipy.interpolate import griddata\n",
    "import matplotlib.colors as colors\n",
    "from matplotlib.colors import LinearSegmentedColormap, BoundaryNorm\n",
    "import matplotlib.ticker as ticker\n",
    "plt.rcdefaults()\n",
    "resolution = '50m'\n",
    "category = 'cultural'\n",
    "name = 'admin_0_countries'\n",
    "\n",
    "shpfilename = shapereader.natural_earth(resolution, category, name)\n",
    "\n",
    "df = gpd.read_file(shpfilename)\n",
    "\n",
    "def rect_from_bound(xmin, xmax, ymin, ymax):\n",
    "    \"\"\"Returns list of (x,y)'s for a rectangle\"\"\"\n",
    "    xs = [xmax, xmin, xmin, xmax, xmax]\n",
    "    ys = [ymax, ymax, ymin, ymin, ymax]\n",
    "    return [(x, y) for x, y in zip(xs, ys)]\n",
    "\n",
    "def plot_map(interp_temps, title, label, lower_bound, upper_bound, vcenter, option, cmap_type, save, filename):\n",
    "    # request data for use by geopandas\n",
    "    resolution = '10m'\n",
    "    category = 'cultural'\n",
    "    name = 'admin_0_countries'\n",
    "    data=interp_temps.copy()\n",
    "    shpfilename = shapereader.natural_earth(resolution, category, name)\n",
    "    df = geopandas.read_file(shpfilename)\n",
    "\n",
    "    # get geometry of a country\n",
    "    poly = [df.loc[df['ADMIN'] == 'Sweden']['geometry'].values[0]]\n",
    "\n",
    "    stamen_terrain = cimgt.Stamen('terrain-background')\n",
    "    # define the colormap\n",
    "    if cmap_type=='SPI_cmap':\n",
    "        cmap = colors.ListedColormap(['red', 'orange', 'yellow', 'white', 'lavender', 'plum', 'purple'])\n",
    "    elif cmap_type=='inverted':\n",
    "        cmap='coolwarm_r' #'RdBu'\n",
    "    else:\n",
    "        cmap='coolwarm' #'RdBu_r'\n",
    "\n",
    "    # projections that involved\n",
    "    st_proj = ccrs.PlateCarree()  #projection used by Stamen images\n",
    "    ll_proj = ccrs.PlateCarree()  #CRS for raw long/lat\n",
    "\n",
    "    # create fig and axes using intended projection\n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    ax = fig.add_subplot(1, 1, 1, projection=st_proj)\n",
    "    ax.add_geometries(poly, crs=ll_proj, facecolor='none', edgecolor='black')\n",
    "\n",
    "    pad1 = .1  #padding, degrees unit\n",
    "    exts = [poly[0].bounds[0] - pad1, poly[0].bounds[2] + pad1, poly[0].bounds[1] - pad1, poly[0].bounds[3] + pad1];\n",
    "    ax.set_extent(exts, crs=ll_proj)\n",
    "\n",
    "    # make a mask polygon by polygon's difference operation\n",
    "    # base polygon is a rectangle, another polygon is simplified switzerland\n",
    "    msk = Polygon(rect_from_bound(*exts)).difference( poly[0].simplify(0.01) )\n",
    "    msk_stm  = st_proj.project_geometry (msk, ll_proj)  # project geometry to the projection used by stamen\n",
    "\n",
    "    # get and plot Stamen images\n",
    "    #ax.add_image(stamen_terrain, 8) # this requests image, and plot\n",
    "\n",
    "    # plot the mask using semi-transparency (alpha=0.65) on the masked-out portion\n",
    "    ax.add_geometries( msk_stm, st_proj, zorder=12, facecolor='white', edgecolor='k', alpha=1)\n",
    "\n",
    "    if option=='imshow':\n",
    "        data[data>upper_bound]=upper_bound\n",
    "        data[data<lower_bound]=lower_bound\n",
    "        if cmap_type=='SPI_cmap':\n",
    "            bounds=[-3.0, -2.0, -1.5, -1.0, 1.0, 1.5, 2.0, 3.0]\n",
    "            norm = colors.BoundaryNorm(bounds, cmap.N)\n",
    "            im = ax.imshow(data, origin='lower', norm=norm,\n",
    "                extent=[lon_grid.min(), lon_grid.max(), lat_grid.min(), lat_grid.max()],\n",
    "                cmap=cmap, zorder=11)\n",
    "        else:\n",
    "            # Create bounds vector from 10th to 90th percentile with median in the center\n",
    "            # Calculate the distance between center, lower and upper bounds\n",
    "            lb=lower_bound\n",
    "            ub=upper_bound\n",
    "            dist = vcenter - lb\n",
    "            dist2 = ub - vcenter\n",
    "            # Create bounds vector from 10th to 90th percentile with median closer to p10\n",
    "            bounds = [lb, lb + 0.25*dist , lb + 0.4*dist, lb + 0.5*dist, lb + 0.7*dist, lb + 0.8*dist, lb + 0.9*dist, lb + dist, vcenter + 0.25*dist2, vcenter + 0.5*dist2, vcenter + 0.75*dist2, vcenter + 0.8*dist2, vcenter + 0.9*dist2, ub]\n",
    "            im = ax.imshow(data, origin='lower', #norm=BoundaryNorm(bounds, ncolors=256),\n",
    "                vmin= lb, vmax=ub,\n",
    "                extent=[lon_grid.min(), lon_grid.max(), lat_grid.min(), lat_grid.max()],\n",
    "                cmap=cmap, zorder=11)\n",
    "    else:\n",
    "        if cmap_type=='SPI_cmap':\n",
    "            bounds=[-3.0, -2.0, -1.5, -1.0, 1.0, 1.5, 2.0, 3.0]\n",
    "        else:\n",
    "            bounds= np.linspace(lower_bound, upper_bound, num=12)\n",
    "        data[data>upper_bound]=upper_bound\n",
    "        data[data<lower_bound]=lower_bound\n",
    "        # Interpolate missing values\n",
    "        data_interp = pd.DataFrame(data).interpolate(method='linear', axis=0).values\n",
    "        im = ax.contourf(lon_grid, lat_grid, data_interp, levels=bounds,\n",
    "            extent=[lon_grid.min(), lon_grid.max(), lat_grid.min(), lat_grid.max()],\n",
    "            cmap=cmap, vmin=lower_bound, vmax=upper_bound, zorder=11)\n",
    "\n",
    "    # add title\n",
    "    plt.title(title, fontsize=18)\n",
    "    # add a colorbar\n",
    "    cbar_ax = fig.add_axes([0.188, -0.04, 0.623, 0.05]) # [left, bottom, width, height]\n",
    "    num_ticks = 7  # Adjust this value as needed\n",
    "    tick_locator = ticker.MaxNLocator(nbins=num_ticks)\n",
    "    tick_formatter = ticker.ScalarFormatter(useMathText=True)\n",
    "    cbar = plt.colorbar(im, cax=cbar_ax, orientation='horizontal', shrink=1.2,\n",
    "                        ticks=tick_locator, format=tick_formatter, extend='both')\n",
    "    #plt.subplots_adjust(top=0.95, bottom=0.15, left=0.0, right=1, hspace=0.2, wspace=0.2)\n",
    "    cbar.ax.tick_params(labelsize=12)\n",
    "    cbar.set_label(label, fontsize=14)\n",
    "    if save=='yes':\n",
    "        fig.savefig('figures/my_figure.png', dpi=300, bbox_inches='tight')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6051421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read xarray of Flood indexes\n",
    "import xarray as xr\n",
    "fidx_ds = xr.open_dataset('Fidx.nc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e396166",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "lat_grid = np.linspace(55.3376, 69.1763, 100) \n",
    "lon_grid = np.linspace(11, 24.113, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94acc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot maximum flood index\n",
    "fidx_max = np.nanmax(fidx_ds['flood_index'].values, axis=0) # maximum flood index\n",
    "plot_map(fidx_max, 'Maximum Daily Flood Index (1922-2021)', 0, 4, 2, 'imshow', 'normal','no', 'no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0489f926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert time coordinate to datetime type\n",
    "fidx_ds['time'] = pd.to_datetime(fidx_ds.time.values)\n",
    "\n",
    "# set time coordinate as index\n",
    "fidx_ds = fidx_ds.set_index(time='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5068e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample the pandas dataframe to monthly frequency and take the max value for each month\n",
    "fidx_ds_mon = fidx_ds.resample(time='M').max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a504b887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import linregress\n",
    "import xarray as xr\n",
    "# Plot Daily Flood Index\n",
    "# convert dataset to dataframe\n",
    "fidx_df = fidx_ds_mon.to_dataframe().reset_index()\n",
    "# plot the data\n",
    "plt.figure(figsize=(14,6))\n",
    "sns.set_style('whitegrid')\n",
    "for lat, lon in zip(fidx_df['lat'].unique(), fidx_df['lon'].unique()):\n",
    "    fidx_df_subset = fidx_df[(fidx_df['lat'] == lat) & (fidx_df['lon'] == lon)]\n",
    "    x = fidx_df_subset['time']\n",
    "    y = fidx_df_subset['flood_index']\n",
    "    plt.plot(x, y, color='lightgray', alpha=0.1, linewidth=0.5)    \n",
    "avg_fidx_df = fidx_df.groupby('time').mean().reset_index()\n",
    "slope, intercept, r_value, p_value, std_err = linregress(avg_fidx_df.index.values, avg_fidx_df['flood_index'])\n",
    "trend_line = slope * avg_fidx_df.index.values + intercept\n",
    "plt.plot(avg_fidx_df['time'], avg_fidx_df['flood_index'], color='black', linewidth=2, label='Average')\n",
    "plt.plot(avg_fidx_df['time'], trend_line, color='red', linestyle='--', linewidth=2, label='Slope: '+str(round(slope,4)))\n",
    "plt.ylim([-3, 3])\n",
    "\n",
    "plt.xlabel('Time', fontsize=14)\n",
    "plt.ylabel('Maximum Monthly Daily Flood Index', fontsize=14)\n",
    "#plt.title('SPEI12 Timeseries', fontsize=14)\n",
    "# Set the font size for tick labels\n",
    "plt.tick_params(axis='both', labelsize=12)\n",
    "#plt.legend(loc='upper center', ncol=10)\n",
    "plt.legend(fontsize=14, loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig('Fidx.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01183e1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa37ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_floods(time, idx_point):\n",
    "    # initialize output arrays\n",
    "    flood_days = np.zeros_like(idx_point)\n",
    "    duration = np.zeros_like(idx_point)\n",
    "    intensity = np.zeros_like(idx_point)\n",
    "    frequency = np.zeros_like(idx_point)\n",
    "\n",
    "    # find floods\n",
    "    flood_start = -1\n",
    "    num_floods = 0\n",
    "    num_extreme_floods = 0 # new counter for extreme floods\n",
    "    current_year = int(str(time[0].astype('M8[Y]')).split('-')[0])\n",
    "    for i, (fidx, dt) in enumerate(zip(idx_point, time)):\n",
    "        year = int(str(dt.astype('M8[Y]')).split('-')[0])\n",
    "        if year > current_year:\n",
    "            current_year = year\n",
    "            num_floods = 0\n",
    "            num_extreme_floods = 0 # reset counters at the start of each new year\n",
    "\n",
    "        if fidx > 0:\n",
    "            if flood_start == -1:\n",
    "                flood_start = i\n",
    "            flood_days[i] = 1\n",
    "        else:\n",
    "            if flood_start != -1:\n",
    "                flood_end = i - 1\n",
    "                flood_duration = flood_end - flood_start + 1\n",
    "                duration[flood_start] = flood_duration\n",
    "                flood_intensity = np.sum(idx_point[flood_start:flood_end+1])\n",
    "                intensity[flood_start] = flood_intensity\n",
    "                flood_peak = np.min(idx_point[flood_start:flood_end+1])\n",
    "                if flood_peak>=0: # # count all floods\n",
    "                    frequency[flood_start] = num_extreme_floods + 1 # increment the frequency counter\n",
    "                    num_extreme_floods += 1 # increment the counter for extreme floods only\n",
    "                num_floods += 1 # always increment the total flood counter\n",
    "                flood_start = -1\n",
    "\n",
    "    # handle last flood if it exists\n",
    "    if flood_start != -1:\n",
    "        flood_end = len(idx_point) - 1\n",
    "        flood_duration = flood_end - flood_start + 1\n",
    "        duration[flood_start] = flood_duration\n",
    "        flood_intensity = np.sum(idx_point[flood_start:flood_end+1])\n",
    "        intensity[flood_start] = flood_intensity\n",
    "        flood_peak = np.min(idx_point[flood_start:flood_end+1])\n",
    "        if flood_peak>=0: # count all floods\n",
    "            frequency[flood_start] = num_extreme_floods + 1 # increment the frequency counter\n",
    "            num_extreme_floods += 1 # increment the counter for extreme floods only\n",
    "\n",
    "    return duration, intensity, flood_days, frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffa2bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return array[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c105064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check flood information\n",
    "lat_near= find_nearest(fidx_ds.lat.values, 68)\n",
    "lon_near= find_nearest(fidx_ds.lon.values, 18.1)\n",
    "time = fidx_ds.sel(lat=lat_near, lon=lon_near)['time'].values\n",
    "fidx_values = fidx_ds.sel(lat=lat_near, lon=lon_near)['flood_index'].values\n",
    "duration, intensity, flood_days, frequency = find_floods(time, fidx_values)\n",
    "sample_df=pd.DataFrame(columns=['time','flood_index','duration','intensity','flood_days','frequency'])\n",
    "sample_df['time']=time\n",
    "sample_df['flood_index']=fidx_values\n",
    "sample_df['duration']=duration\n",
    "sample_df['intensity']=intensity\n",
    "sample_df['flood_days']=flood_days\n",
    "sample_df['frequency']=frequency\n",
    "#sample_df.to_excel('Flood detection check.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4d4156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect flood duration, intensity and frequency\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Compute drought intensity, duration and frequency at all points and save results\n",
    "fidx_ds = xr.open_dataset('fidx.nc')\n",
    "# convert time coordinate to datetime type\n",
    "fidx_ds['time'] = pd.to_datetime(fidx_ds.time.values)\n",
    "# set time coordinate as index\n",
    "fidx_ds = fidx_ds.set_index(time='time')\n",
    "\n",
    "lat_grid = np.linspace(55.3376, 69.1763, 100) \n",
    "lon_grid = np.linspace(11, 24.113, 100)\n",
    "\n",
    "# Create empty dataset with desired dimensions and coordinates\n",
    "ds = xr.Dataset(\n",
    "    {\n",
    "        'fl_durs': (['time', 'lat', 'lon'], np.zeros((len(fidx_ds.time.values), 100, 100))),\n",
    "        'fl_ints': (['time', 'lat', 'lon'], np.zeros((len(fidx_ds.time.values), 100, 100))),\n",
    "        'fl_days': (['time', 'lat', 'lon'], np.zeros((len(fidx_ds.time.values), 100, 100))),\n",
    "        'fl_freq': (['time', 'lat', 'lon'], np.zeros((len(fidx_ds.time.values), 100, 100))),\n",
    "    },\n",
    "    coords={\n",
    "        'time': fidx_ds.time.values,\n",
    "        'lat': lat_grid,\n",
    "        'lon': lon_grid,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Fill in dataset with drought data\n",
    "with tqdm(total=(len(lat_grid)*len(lon_grid))) as pbar:\n",
    "    for i in range(len(lat_grid)):\n",
    "        #print(i)\n",
    "        for j in range(len(lon_grid)):\n",
    "            #print(j)\n",
    "            fidx_values = fidx_ds.flood_index.values[:, i, j] # timeseries at point i,j\n",
    "            #print(np.nanmax(fidx_ds_values))\n",
    "            duration, intensity, flood_days, frequency = find_floods(time, fidx_values)\n",
    "            ds['fl_durs'][:,i,j] = duration\n",
    "            ds['fl_ints'][:,i,j] = intensity\n",
    "            ds['fl_days'][:,i,j] = flood_days\n",
    "            ds['fl_freq'][:,i,j] = frequency\n",
    "            pbar.update(1) \n",
    "# Save dataset as netCDF file\n",
    "ds.to_netcdf('flood_data.nc')\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34dbd21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4eb8abc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45707d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b660c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_ds = xr.open_dataset('flood_data.nc')\n",
    "fl_ds_yearly =fl_ds.resample(time='1Y').sum('time') \n",
    "#fl_ds_decade =fl_ds_yearly.resample(time='10Y').sum('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642b4046",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi_ds = xr.open_dataset('Fidx.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a204bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'time' dtype to datetime64[ns]\n",
    "dfi_ds['time'] = xr.DataArray(pd.to_datetime(dfi_ds.time.values))\n",
    "\n",
    "# Verify the updated dtype\n",
    "print(dfi_ds.time.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6d6ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Specify the day, month and year of interest\n",
    "target_day = 8\n",
    "target_month = 6\n",
    "target_year = 1938\n",
    "\n",
    "# Convert target day, month, and year to numpy.datetime64 objects\n",
    "target_date = np.datetime64(f'{target_year}-{target_month:02}-{target_day:02}')\n",
    "\n",
    "fl_time = dfi_ds['time']\n",
    "delta_time = np.abs((fl_time - target_date).astype('timedelta64[M]'))\n",
    "t = delta_time.argmin()\n",
    "\n",
    "# Extract day, month, and year from the selected time index\n",
    "selected_time = dfi_ds.time.values[t]\n",
    "day = np.datetime_as_string(selected_time, unit='D')\n",
    "month = np.datetime_as_string(selected_time, unit='M')\n",
    "year = np.datetime_as_string(selected_time, unit='Y')\n",
    "\n",
    "# Call the plot_map function with the corrected time values\n",
    "plot_map(dfi_ds['flood_index'].values[t,:], 'DFI', f'{day}', -3, 1, 0, 'imshow', 'DFI_cmap', 'no', 'noname')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868352eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_ds_yearly2 =fl_ds.resample(time='1Y').max('time') # for counting frequency of floods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15945e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to calculate percentiles along the time dimension\n",
    "def calc_percentiles(data):\n",
    "    return np.nanpercentile(data, [10, 50, 90], axis=0)\n",
    "fl_durs_np = fl_ds_yearly.fl_durs.values \n",
    "#fl_durs_np[fl_durs_np==0]=np.nan \n",
    "fl_ints_np = fl_ds_yearly.fl_ints.values \n",
    "#fl_ints_np[fl_ints_np==0] = np.nan  # deactivated, including 0s now\n",
    "#fl_freq_np = fl_ds_yearly.fl_days.values # number of days/pear under flood condition\n",
    "fl_freq_np = fl_ds_yearly2.fl_freq.values # number of flood events\n",
    "#fl_freq_np = np.nan_to_num(fl_freq_np, nan=0)\n",
    "dur_perc_np = calc_percentiles(fl_durs_np)\n",
    "int_perc_np = calc_percentiles(fl_ints_np)\n",
    "freq_perc_np = calc_percentiles(fl_freq_np)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098002f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot strongest intensities\n",
    "#plot_data=np.nanmax(fl_ds.fl_ints.values, axis=0)\n",
    "plot_data=int_perc_np[2]\n",
    "lb=int(np.percentile(plot_data[plot_data>0], 0.01))\n",
    "ub=int(np.percentile(plot_data[plot_data>0], 99.99))\n",
    "vcenter=int(np.percentile(plot_data[plot_data>0], 50))\n",
    "ub=18\n",
    "#lb=10\n",
    "#vcenter=0.5\n",
    "#plot_map(plot_data, '90th Percentile Yearly Flood Intensity', lb, ub, vcenter, 'imshow', 'normal','no','no')\n",
    "plot_map(plot_data, 'Flood Danger Intensity Hotspots', 'Top decile of annual cumulative flood danger intensity', lb, ub, vcenter, 'imshow', 'normal','no','no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed17f137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot longest durations\n",
    "#plot_data=np.nanmax(dr_durs_np, axis=0)/12\n",
    "plot_data=dur_perc_np[2]#/12\n",
    "lb=int(np.percentile(plot_data[plot_data>0], 5))\n",
    "ub=int(np.percentile(plot_data[plot_data>0], 95))\n",
    "vcenter=int(np.percentile(plot_data[plot_data>0], 50))\n",
    "ub=50\n",
    "#lb=35\n",
    "#vcenter=5\n",
    "#plot_map(plot_data, '90th Percentile Yearly Flood Condition Duration (days)', lb, ub, vcenter, 'imshow', 'normal','no','no')\n",
    "plot_map(plot_data, 'Flood Danger Duration Hotspots', 'Top decile of annual cumulative flood duration (days)', lb, ub, vcenter, 'imshow', 'normal','no','no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4799b07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot days under flood conditions\n",
    "#plot_data=freq_perc_np[2] # 90th percentile of droughts per decade or year depending on previous cell\n",
    "plot_data=np.nansum(fl_ds_yearly.fl_freq.values, axis=0)\n",
    "lb=int(np.percentile(plot_data[plot_data>0], 5))\n",
    "ub=int(np.percentile(plot_data[plot_data>0], 95))\n",
    "vcenter=int(np.percentile(plot_data[plot_data>0], 50))\n",
    "#ub=100\n",
    "#lb=70\n",
    "#vcenter=2\n",
    "plot_map(plot_data, 'Total Number Flood Condition Events (DFI)', lb, ub, vcenter, 'imshow', 'normal','no','no')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f14db6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3d1e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot days under flood conditions\n",
    "#plot_data=freq_perc_np[2] # 90th percentile of droughts per decade or year depending on previous cell\n",
    "plot_data=np.nansum(fl_ds_yearly.fl_days.values, axis=0)\n",
    "lb=int(np.percentile(plot_data[plot_data>0], 5))\n",
    "ub=int(np.percentile(plot_data[plot_data>0], 95))\n",
    "vcenter=int(np.percentile(plot_data[plot_data>0], 50))\n",
    "#ub=100\n",
    "#lb=400\n",
    "#vcenter=2\n",
    "plot_map(plot_data, 'Total Number of Days Under Flood Condition (DFI)', lb, ub, vcenter, 'imshow', 'normal','no','no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecb6cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot days under flood conditions\n",
    "#plot_data=freq_perc_np[2] # 90th percentile of droughts per decade or year depending on previous cell\n",
    "values=fl_ds_yearly.fl_days.values\n",
    "#values[values==0]=np.nan # Consider non danger days or not\n",
    "plot_data=np.nansum(values, axis=0)#/100\n",
    "lb=int(np.percentile(plot_data[plot_data>0], 5))\n",
    "ub=int(np.percentile(plot_data[plot_data>0], 95))\n",
    "vcenter=int(np.percentile(plot_data[plot_data>0], 50))\n",
    "#ub=100\n",
    "#lb=400\n",
    "#vcenter=2\n",
    "#plot_map(plot_data, 'Flood Condition Frequency (days/year)', lb, ub, vcenter, 'imshow', 'normal','no','no')\n",
    "plot_map(plot_data, 'Flood Danger Frequency (Days per Year)', 'Total Values', lb, ub, vcenter, 'imshow', 'normal','no','no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d85db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot longest durations\n",
    "plot_data=freq_perc_np[2]#/12\n",
    "lb=int(np.percentile(plot_data[plot_data>0], 5))\n",
    "ub=int(np.percentile(plot_data[plot_data>0], 95))\n",
    "vcenter=int(np.percentile(plot_data[plot_data>0], 50))\n",
    "ub=9\n",
    "plot_map(plot_data, 'Flood Danger Frequency Hotspots', 'Top decile of annual count of flood danger events', lb, ub, vcenter, 'imshow', 'normal','no','no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583eff06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot days under flood conditions\n",
    "plot_data=freq_perc_np[2] # 90th percentile of droughts per decade or year depending on previous cell\n",
    "lb=int(np.percentile(plot_data[plot_data>0], 5))\n",
    "ub=int(np.percentile(plot_data[plot_data>0], 95))\n",
    "vcenter=int(np.percentile(plot_data[plot_data>0], 50))\n",
    "ub=50\n",
    "plot_map(plot_data, '90th Percentile Yearly Flood Condition Frequency', lb, ub, vcenter, 'imshow', 'normal','no','no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835622af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr, linregress\n",
    "\n",
    "def calculate_trends(data_array, alpha=0.05):\n",
    "    timesteps, n_lat, n_lon = data_array.shape\n",
    "\n",
    "    x = np.arange(timesteps)\n",
    "    rate_of_change = np.empty((n_lat, n_lon))\n",
    "\n",
    "    for i in range(n_lat):\n",
    "        for j in range(n_lon):\n",
    "            y = data_array[:, i, j]\n",
    "            mask = ~np.isnan(y)\n",
    "            if mask.sum() > 1:\n",
    "                slope, intercept, r_value, p_value, std_err = linregress(x[mask], y[mask])\n",
    "                if p_value <= alpha:\n",
    "                    rate_of_change[i, j] = slope\n",
    "                else:\n",
    "                    rate_of_change[i, j] = np.nan\n",
    "            else:\n",
    "                rate_of_change[i, j] = np.nan\n",
    "\n",
    "    return rate_of_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423aae62",
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_dur_rate=calculate_trends(fl_ints_np) # Total duration per year\n",
    "plot_map(fl_dur_rate, 'Flood Danger Intensity Trends', 'Annual change of cumulative flood danger intensity',-0.3, 0.3, 0, 'imshow', 'normal','no','no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cf721d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_dur_rate=calculate_trends(fl_durs_np) # Total duration per year, not considering zeroes\n",
    "plot_map(fl_dur_rate, 'Flood Danger Duration Trends', 'Annual change of cumulative flood danger duration (days)',-0.3, 0.3, 0, 'imshow', 'normal','no','no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08eeb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_freq_rate=calculate_trends(fl_freq_np) \n",
    "plot_map(fl_freq_rate, 'Flood Danger Frequency Trends','Annual change of flood danger event count', -0.1, 0.1, 0, 'imshow', 'normal','no','no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bee666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr, linregress\n",
    "\n",
    "def calculate_corr(data_array, data_array2):\n",
    "    timesteps, n_lat, n_lon = data_array.shape\n",
    "    corr = np.empty((n_lat, n_lon))\n",
    "    for i in range(n_lat):\n",
    "        for j in range(n_lon):\n",
    "            x = data_array2[:, i, j]\n",
    "            y = data_array[:, i, j]\n",
    "            mask = ~np.isnan(y)\n",
    "            if mask.sum() > 1 and np.unique(x).size != 1:\n",
    "                slope, intercept, r_value, p_value, std_err = linregress(x[mask], y[mask])\n",
    "                corr[i, j] = r_value\n",
    "            else:\n",
    "                corr[i, j] = np.nan\n",
    "\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dd9811",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_dur_cor=calculate_corr(fl_ints_np, fl_durs_np)\n",
    "plot_map(int_dur_cor, 'Flood Danger Correlations','Correlation coefficient between intensity and duration', 0, 1, 0, 'imshow', 'normal','no','no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f58236",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_map(int_freq_cor, 'Flood Danger Correlations','Correlation coefficient between intensity and frequency', 0, 1, 0, 'imshow', 'normal','no','no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c38ea3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615cf471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "dfi_ds=xr.open_dataset('Fidx.nc') # Previously calculated Flood Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25b26d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e076d5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_ds_yearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11127106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot national average number of days under flood conditions \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import linregress\n",
    "import xarray as xr\n",
    "# Plot Daily Flood Index\n",
    "# convert dataset to dataframe\n",
    "fday_df = fl_ds_yearly.to_dataframe().reset_index()\n",
    "# plot the data\n",
    "plt.figure(figsize=(14,6))\n",
    "sns.set_style('whitegrid')\n",
    "for lat, lon in zip(fday_df['lat'].unique(), fday_df['lon'].unique()):\n",
    "    fday_df_subset = fday_df[(fday_df['lat'] == lat) & (fday_df['lon'] == lon)]\n",
    "    x = fday_df_subset['time']\n",
    "    y = fday_df_subset['fl_days']\n",
    "    #plt.plot(x, y, color='lightgray', alpha=0.1, linewidth=0.5)    \n",
    "avg_fday_df = fday_df.groupby('time').mean().reset_index()\n",
    "slope, intercept, r_value, p_value, std_err = linregress(avg_fday_df.index.values, avg_fday_df['fl_days'])\n",
    "trend_line = slope * avg_fday_df.index.values + intercept\n",
    "plt.plot(avg_fday_df['time'], avg_fday_df['fl_days'], color='black', linewidth=2, label='Spatial Average')\n",
    "plt.plot(avg_fday_df['time'], trend_line, color='red', linestyle='--', linewidth=2, label='Slope: '+str(round(slope,4)))\n",
    "plt.ylim([0, 35])\n",
    "\n",
    "plt.xlabel('Time', fontsize=14)\n",
    "plt.ylabel('Number of Days per Year Under Flood Conditions', fontsize=14)\n",
    "#plt.title('SPEI12 Timeseries', fontsize=14)\n",
    "# Set the font size for tick labels\n",
    "plt.tick_params(axis='both', labelsize=12)\n",
    "#plt.legend(loc='upper center', ncol=10)\n",
    "plt.legend(fontsize=14, loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig('fdays.svg')\n",
    "plt.show()\n",
    "obs1_avg=round(np.nanmean(avg_fday_df['fl_days'].iloc[0:50]), 2)\n",
    "obs2_avg=round(np.nanmean(avg_fday_df['fl_days'].iloc[51:101]), 2)\n",
    "proj_avg=round(np.nanmean(slope * (avg_fday_df.index.values[0:50]+101) +intercept), 2)\n",
    "print(f\"Average from 1922-1971: {obs1_avg}\")\n",
    "print(f\"Average from 1972-2021: {obs2_avg}\")\n",
    "print(f\"Projected Average from 2022-2071: {proj_avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0660413e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask out values outside country boundaries\n",
    "lat_grid = np.linspace(55.3376, 69.1763, 100) \n",
    "lon_grid = np.linspace(11, 24.113, 100)\n",
    "from shapely.geometry import Polygon, Point\n",
    "from cartopy.io import shapereader\n",
    "import geopandas\n",
    "def rect_from_bound(xmin, xmax, ymin, ymax):\n",
    "    \"\"\"Returns list of (x,y)'s for a rectangle\"\"\"\n",
    "    xs = [xmax, xmin, xmin, xmax, xmax]\n",
    "    ys = [ymax, ymax, ymin, ymin, ymax]\n",
    "    return [(x, y) for x, y in zip(xs, ys)]\n",
    "# Mask from natural earth (more below)\n",
    "# request data for use by geopandas\n",
    "resolution = '50m'\n",
    "category = 'cultural'\n",
    "name = 'admin_0_countries'\n",
    "\n",
    "shpfilename = shapereader.natural_earth(resolution, category, name)\n",
    "df = geopandas.read_file(shpfilename)\n",
    "\n",
    "# get geometry and polygon of a country\n",
    "poly = [df.loc[df['ADMIN'] == 'Sweden']['geometry'].values[0]]\n",
    "exts = [min(lon_grid)-1, max(lon_grid)+1, min(lat_grid)-1, max(lat_grid)+1]\n",
    "msk = Polygon(rect_from_bound(*exts)).difference( poly[0].simplify(0.01) )\n",
    "# create boolean mask where True=inside polygon\n",
    "mask=np.zeros((len(lat_grid),len(lon_grid)),dtype=bool)\n",
    "for i, y in enumerate(lat_grid):\n",
    "    for j, x in enumerate(lon_grid):\n",
    "        mask[i,j]=Point(x,y).within(msk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254df93d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49632a64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
